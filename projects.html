<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/media/5d7fcf765059a344-s.p.otf" as="font" type="font/otf" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/ac7c9f14c7799d15-s.p.otf" as="font" type="font/otf" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/535c141938fc81c3-s.p.otf" as="font" type="font/otf" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/e11418ac562b8ac1-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/785a3aece39c3f1a.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/785a3aece39c3f1a.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-66d32731bdd20e83.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-06a389466d83cb87.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-6fda02e482645a0c.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/projects-2fcf7aafd7c38644.js" defer="" crossorigin=""></script><script src="/_next/static/35x8J-pMSzD2W4df1IM-v/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/35x8J-pMSzD2W4df1IM-v/_ssgManifest.js" defer="" crossorigin=""></script></head><body class="bg-[black]"><div id="__next"><style data-mantine-styles="classes">@media (max-width: 35.99375em) {.mantine-visible-from-xs {display: none !important;}}@media (min-width: 36em) {.mantine-hidden-from-xs {display: none !important;}}@media (max-width: 47.99375em) {.mantine-visible-from-sm {display: none !important;}}@media (min-width: 48em) {.mantine-hidden-from-sm {display: none !important;}}@media (max-width: 61.99375em) {.mantine-visible-from-md {display: none !important;}}@media (min-width: 62em) {.mantine-hidden-from-md {display: none !important;}}@media (max-width: 74.99375em) {.mantine-visible-from-lg {display: none !important;}}@media (min-width: 75em) {.mantine-hidden-from-lg {display: none !important;}}@media (max-width: 87.99375em) {.mantine-visible-from-xl {display: none !important;}}@media (min-width: 88em) {.mantine-hidden-from-xl {display: none !important;}}</style><main class="__variable_56c696 __variable_3a0388"><div class="" style="position:fixed;top:0;left:0;height:2px;background:transparent;z-index:99999999999;width:100%"><div class="" style="height:100%;background:rgba(255,255,255,0.8);transition:all 500ms ease;width:0%"><div style="box-shadow:0 0 10px rgba(255,255,255,0.8), 0 0 10px rgba(255,255,255,0.8);width:5%;opacity:1;position:absolute;height:100%;transition:all 500ms ease;transform:rotate(2deg) translate(0px, -2px);left:-10rem"></div></div></div></main></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"allPosts":[{"title":"3D Parametric curves and surfaces for computer visualization","date":"2024-10-18","slug":"parametric_surface","author":{"name":"Kim Dong Hun"},"excerpt":"When it comes to programatically visualizing an equation in a 3D space, the ability to decompose the equation into parametric equations is key. Based on how we parametricize the equation, we can have a curve or either a surface. The resulting curves or surfaces are called parametric curves or surfaces.","keyword":"Parametric Curves","content":"\n## Representing a curve in $\\mathbb{R}^3$\n\nWe all know how to create a line or a curve in $\\mathbb{R}^2$ using the equation $y = f(x)$. We just need to have a single equation that have two variables in it. What happens if you want to represent a curve in $\\mathbb{R}^3$? We can use the concept of parametric curves to represent a curve in $\\mathbb{R}^3$.\n\nHowever, creating a curve in $\\mathbb{R}^3$ is not as simple as creating a curve in $\\mathbb{R}^2$. Yes, if the function is simple enough like $z = x^2 + y^2$, then we can easily represent the curve in $\\mathbb{R}^3$. But what if we were given two implicit equations instead and required to find the intersecting line, such as $x^2 + y^2 = 1$ and $y=xz$? Finding a curve to represent the intersection of these two equations is not as simple as finding the intersection of two lines in $\\mathbb{R}^2$.\n\nIn fact, if you think this problem in a practical point of view, this is in fact how many of our social science models are introduced. We do not have a single equation that represent the behavior of the system. Instead, we have multiple equations that can represent the behavior of the system. Thus, being able to solve the intersection of these equations is a very important skill to have.\n\n## Parametric Curves\n\nNow to solve the intersection of the given equations (at least two should be provided for $\\mathbb{R}^3$), we can use the concept of parametric curves. The idea is to represent the equations with a common variable. For example, let us say we want to parameterize $x^2 + y^2 = 1$ and $y=xz$. At first glance, it seems impossible to represent the intersection of these two equations. However, if we use a common variable $t$ to represent the equations and change the euclidean coordinates into a radial coordinate, we can easily represent the intersection of the two equations.\n\n$$\n\\begin{align*}\nx \u0026= 5\\cos(t) \\\\\ny \u0026= 5\\sin(t) \\\\\nz \u0026= 25\\cos(t) \\sin(t)\n\\end{align*}\n$$ \n\nSince all $x$, $y$, and $z$ are represented in terms of $t$, the resulting visualization will be a curve. This is a parametric curve that represents the intersection of the two equations. Now thanks to representing the curve with a single variable, we can easily visualize the curve as a code!\n\n```c++\n// float t is a variable with a range of 0 to 2 * PI\n\nfloat x = 5 * cos(t);\nfloat y = 5 * sin(t);\nfloat z = 25 * cos(t) * sin(t);\n```\n\nI created a glsl shader that visualizes the curve. I first created a plane using Three.js and then created a shader that visualizes the curve. I set the t as `position.x` and then calculated the x, y, and z values using the equations above. The result is a beautiful curve that represents the intersection of the two equations.\n\n\u003cParametricSurface1 /\u003e\n\n## Parametric Surface\n\nThere are some cases we are not given sufficient information to express all $x$, $y$, and $z$ in terms of a single variable. In those cases, we would need two variables to parametricize the equations. Since the variable dimension is now 2, instead of having a curve, we will have a surface. The resulting surface is called a parametric surface. In most cases, the variables we will parameterize on are named $u$ and $v$. An example that requires us to use two variables to parametricize the equations would be the below equation:\n\n$$\n\\begin{align*}\nx^2 - y^2 + z^2 \u0026= 0 \\\\\n\\end{align*}\n$$\n\nThere is no way we can use a single variable to parametricize the equation. We need to use two variables to parametricize the equation. There are two ways you can parametricize the equation. One way is to think of x and y being the two variables that we will use to parametricize the equation. Then we will have the following equations:\n\n$$\n\\begin{align*}\nx \u0026= u \\\\\ny \u0026= v \\\\\nz \u0026= \\pm\\sqrt{u^2 - v^2}\n\\end{align*}\n$$\n\nHowever, the above equation is problematic in that we need to use the $\\pm$ sign to represent the equation. In terms of coding, this will mean that we need to create two curves to represent the equation. We can solve this by parametricizing the equation in a different way, which is to use radial coordinates.\n\n\u003e In most cases, in the case of $\\mathbb{R}^3$, when there are two squared terms that are added together, we can use radial coordinates to parametricize the equation. In the case of the equation $x^2 - y^2 + z^2 = 0$, we can change x and z into radial coordinates since they are squared terms that are added together.\n\n\u003e Remember that changing a cartisian coordinate into a radial coordinate is done by using the following equations: $x = r\\cos(u)$, $y = r$, and $z = r\\sin(u)$.\n\n$$\n\\begin{align*}\nx \u0026= v\\cos(u) \\\\\ny \u0026= v \\\\\nz \u0026= v\\sin(u)\n\\end{align*}\n$$\n\nThe reason why we add an v in front of the $\\cos$ and $\\sin$ functions is because according to the equation $x^2 - y^2 + z^2 = 0$, the radius of the curve is $y$. Now with the parametricized equations expressed in terms of u and v, we can easily visualize the curve using the code below:\n\n```c++\n// float u and v are variables with a range of 0 to 2 * PI\nfloat x = v * cos(u);\nfloat y = v;\nfloat z = v * sin(u);\n```\n\n\u003cParametricSurface2 /\u003e\n\nSimilar to the above coding, I used the x and y in the plane geometry of Three.js as u and v and then calculated the x, y, and z values using the equations above.\n\nWe can creatively experiment on new forms such as a pasta with the correct equations. We can visualize a pasta using a parametric surface described with radial coordinates. \n\n$$\n\\begin{align*}\nr = 15 - 9 \\cos(x/3) \\\\\n\\end{align*}\n$$\n\nWith the above equation, since $x$ is in the equation already, it would be nice if we could think of $x$ as $u$. We can think of the $\\theta$ as $v$ since it is not given. Then we can parametricize the equation as follows:\n\n$$\n\\begin{align*}\nx \u0026= u \\\\\ny \u0026= (15 - 9 \\cos(u/3)) \\sin(v) \\\\\nz \u0026= (15 - 9 \\cos(u/3)) \\cos(v) \\\\\n\\end{align*}\n$$\n\nThe resulting visualization is a pasta that is represented by the parametric surface.\n\n\u003cParametricSurface3 /\u003e\n\n\n\nParametric curves and surfaces are an interesting concept that is good to have in your toolbox especially for computer graphics and art. If it is difficult to imagine why we need to learn parametricization, just simply remember that it is through parametricization that we can code the surfaces in software!\n","categories":["Geometry","Mathematics"],"thumbnail":"/assets/posts/parametric_surface/pasta_surface.jpg","WIP":false,"data":{"title":"3D Parametric curves and surfaces for computer visualization","excerpt":"When it comes to programatically visualizing an equation in a 3D space, the ability to decompose the equation into parametric equations is key. Based on how we parametricize the equation, we can have a curve or either a surface. The resulting curves or surfaces are called parametric curves or surfaces.","date":"2024-10-18","author":{"name":"Kim Dong Hun"},"keyword":"Parametric Curves","categories":["Geometry","Mathematics"],"WIP":false,"thumbnail":"/assets/posts/parametric_surface/pasta_surface.jpg"}},{"title":"Linear Regression Model Selection (feat. Geomertry)","date":"2024-10-10","slug":"linear_regression_geometry","author":{"name":"Kim Dong Hun"},"excerpt":"Linear Regression Model is a powerful tool for modeling multiple variables and their relationships to the targeted output variable. However, having many predictors can overcomplicate the model and output poor results. We will understand linear regression model selection methods (Ridge, Lasso) in a geometric perspective.","keyword":"Model Selection","content":"\n## Linear Regression Fails with Many Predictors\n\nLinear regression is powerful in that it allows us \"humans\" to understand what is happening in the model and how the model reasoned to make a prediction. This is because Linear regression essetially outputs a linear equation like below:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n\n$$\n\nwhere $y$ is the output variable, $x_1, x_2, \\cdots, x_n$ are the input variables, and $\\beta_0, \\beta_1, \\cdots, \\beta_n$ are the coefficients of the input variables.\n\nHowever, it gets complicated when we are trying to introduce many input variables, sometimes more variables than the number of observed samples. In such case, simple linear regression model that uses all the input variables (Ordinary Least Squares) can overfit the data and output poor results.\n\n## Model Selection with Ridge and Lasso\n\nTo remedy cases where we have many input variables, there are mainly three methods we can use to create another linear regression model.\n\n1. **Forward and Backward Selection**: This method selects the best subset of input variables that can explain the output variable. It is a brute-force method that tries all possible combinations of input variables and selects the best subset.\n\n2. **Shrinkage method**: This method shrinks the coefficients of the input variables to zero. This method is useful when we have many input variables and we want to select only the important variables. There are two types of shrinkage methods: Ridge and Lasso.\n\n3. **Dimension Reduction**: This method reduces the dimension of the input variables. This method is useful when we have many input variables and we want to reduce the number of variables. One example of this would be Primary Component Analysis (PCA).\n\nIn this post, we will focus mainly on shrinkage method and introduce them not as how it is normally explained but by using geometric images to explain what is happening in them.\n\n## Brief Introduction of Ridge and Lasso\n\nTo first understand Ridge and Lasso, one should have a good understanding of Ordinary Least Squares, which is the most basic version of linear regression model.\n\nThe ultimate goal of Ordinary Least Squares is to find the coefficients ($\\beta_0, \\beta_1, \\cdots, \\beta_n$) of the predictors that make the residual sum of squares (RSS) to 0. The Residual sum of squares is calculated as below:\n\n$$\nRSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\nwhere $y_i$ is the actual output value and $\\hat{y}_i$ is the predicted output value.\n\nSince OLS goal is just to minimize the difference between the actual and predicted output values, it can be said that OLS has the least bias.\n\n### OLS and MSE?\n\nOK. Now frankly speaking, many already know MSE (Mean Squared Error) since it is a term that appears both in statistics and machine learning. But what is OLS? It seems that they are connected but how?\n\nSo Ordinary Least Squares (OLS) can be best understand when contrasted with other methods that have similar names. Other than OLS, there is also a a WLS which is Weighted Least Squares. So, both have least squares in their names and essentially they compare between the actual and predicted values whose difference is squared for later minimization. However, WLS applys weights to the squared differences to specific samples. Essentially its goal is to minimize the weighted sum of squared differences:\n\n$$\nMinimize \\sum_{i=1}^{n} w_i(y_i - \\hat{y}_i)^2\n$$\n\nwhere $w_i$ is the weight applied to the $i$-th sample.\n\nOn the other hand, OLS does not apply any weights to the squared differences. It just tries to minimize the sum of squared differences:\n\n$$\nMinimize \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\nIn fact, the 'Ordinary' in OLS means that it does not apply any weights to the squared differences. It just tries to minimize the sum of squared differences. (For your information $\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$ is also called the Residual Sum of Squares (RSS).)\n\nSo how does Mean Squared Error (MSE) come into play? Well, MSE is essentially the average of the squared differences between the actual and predicted values. So it can be said that MSE is the average of the RSS. Thus, it can be said that OLS is the method that tries to minimize the MSE.\n\n$$\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\n### Calculation of Bias and Variance\n\nAllright, so we have been told that OLS has the least bias among linear regression models. Let us talk about the bias and variance of it.\n\nNow when talking about models, bias and variance are two keywords that always show up. Essentially in predictive models, there is always a bias and variance tradeoff. The most famous image that explain both bias and variance is the image below:\n\n![Bias Variance](/assets/posts/linear_regression_geometry/bias_variance.png)\n\nIn the image, the bullseye is the target. The lower the variance, the denser the shots are to each other (regardless of its proximity to the bullseye). The higher the bias, the more offset the shots are from the bullseye. In linear regression, it is a common practice to find a model that has the least bias and variance, which is in fact difficult due to the tradeoff that exists between them.\n\nNow how to calculate the variance is relatively easy. It is the standard deviation of the predicted values themselves. In calculating the variance, there is no need to rely on the actual values. The variance can be calculated as below:\n\n$$\nVar(\\hat{y}) = E[(\\hat{y} - E[\\hat{y}])^2]\n$$\n\n$$\nVar = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - E[\\hat{y}])^2\n$$\n\nwhere $E[\\hat{y}]$ is the expected value of the predicted values.\n\nAs you can see, there is no need to rely on the real observed $y$ values. The variance can be calculated just by using the predicted values.\n\nHowever, bias is a bit more complicated. Bias is the difference between the expected value of the predicted values and the actual values. Thus, it requires us to have our observed $y$ values. The bias can be calculated as below:\n\n$$\nBias(\\hat{y}) = E[\\hat{y}] - y\n$$\n\n$$\nBias = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)\n$$\n\nwhere $y$ is the actual output values.\n\n### OLS has the least bias?\n\nSo, why is OLS said to have the least bias in a mathematical perspective? Let us remember that OLS attempts to minimize the below function\n\n$$\nRSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\nFor exploaratory purposes, we can safely say that minimizing $RSS$ is the same as minimizing $MSE$ which is\n\n$$\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\nIn fact, if we calculate the MSE in $E$ term it is,\n\n$$\nMSE = E[(y - \\hat{y})^2]\n$$\n\nwhich eventually leads to the interpretation of MSE as a function of bias and variance. You may look at how the below function is derived through this [link](https://www.geeksforgeeks.org/bias-vs-variance-in-machine-learning/)\n\n$$\nMSE = Bias^2 + Var + \\sigma^2\n$$\n\nwhere $\\sigma^2$ is the irreducible error.\n\nIf assumptions on the linear model are held, then the bias of the OLS model is 0 according to the Gauss-Markov theorem. The assumptions are 1) the model is linear in the parameters, 2) the errors of residuals are independent and identically distributed, 3) the errors are normally distributed (mean should be 0), and 4) the errors have a constant variance. If these assumptions are held, then the OLS model has the least bias. The specifics of Gauss-Markov theorem can be found in this [link](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem). But just to give you a brief overview, In the final equation of the theorem, the OLS model shows that the mean of the estimated coefficients is equal to the true coefficients.\n\n$$\nE[\\hat{\\beta}] = \\beta\n$$\n\nIf the above is held, the the model can be said to have a bias of 0.\n\n## Ridge Regression\n\nWe have come a long way to really discuss about Ridge Regression and Lasso Regression. So Ridge Regression and Lasso manipulates the bias and variance of the model by adding a penalty term to the OLS model. The penalty term is added to the original RSS and the goal of the Regression model changes to minimizing the below function:\n\n$$\nMinimize \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n$$\n\nSo as you can see here, a new term $\\lambda \\sum_{j=1}^{p} \\beta_j^2$ is added to the RSS function. This term is called the penalty term. The penalty term is the sum of the squares of the coefficients of the input variables. The $\\lambda$ is the hyperparameter that controls the strength of the penalty term. If $\\lambda$ is 0, then the penalty term is 0 and the model is the same as the OLS model. If $\\lambda$ is very large, then the penalty term is very large and the coefficients of the input variables are shrunk to 0.\n\nIn easy words, Ridge Regression introduces a constraint `budget` that the RSS should be operated within. Remind yourself that this is a constraint! The whole point of Ridge Regression is to find the coefficients ($\\beta_0, \\beta_1, \\cdots, \\beta_n$) that minimize the RSS while keeping the sum of the squares of the coefficients within the budget. This is why Ridge Regression is also called L2 regularization.\n\nThis is in fact an optimization problem! We are given a budget and we need to find the point where the RSS is minimized while keeping the sum of the squares of the coefficients within the budget.\n\nLet us understand this easily using geometry. For simplifying the problem let us imagine a two predictor linear regression model. This model will be expressed as below:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n$$\n\nOur goal in Ridge Regression is to find the coefficients ($\\beta_0, \\beta_1, \\beta_2$) that minimize the RSS while keeping the sum of the squares of the coefficients within the budget. But before we go into plotting lines and curves to understand how Ridge Regression works, let us first understand OLS in a geometric perspective.\n\n### OLS in a Geometric Perspective\n\nSince we have a two predictor linear regression model, we can plot the combination of all the coefficients into a 2d plane. OLS model will give us only one single combination in the 2d plane, since there is only one combination that can minimize the RSS in the linear regression model. This is the combination that is closest to the actual output values.\n\n![OLS coefficient combination for two predictors](/assets/posts/linear_regression_geometry/ridge_lasso1.png)\n\n### Ridge Regression in a Geometric Perspective\n\nOLS was relatively easy to understand. Now how about Ridge Regression? First remind that Ridge Regression is minimizing the RSS within a fixed budget. Thus it is an optimization probelm. Our budget can be depicted as an area in the 2d plane. Plus, since our ridge regression is a square of the coefficients, the budget area will be a circle.\n\n![Ridge Regression budget shown as an area](/assets/posts/linear_regression_geometry/ridge_lasso2.png)\n\nNow the dynamic variable is only the RSS. We need to sacrifice RSS to find the right coefficient combination that is within the budget (=the point should fall in the area of the circle). As we sacrifice more and more of the RSS, the coefficients will change. What happens if we sacrifice RSS? The RSS will be a value higher than 0 and thus, many coefficients can satisfy to meet such RSS. For instance for a RSS=1, the coefficient combinations will be shown as a line in the 2d plane. All combination points on the line will have a RSS of 1.\n\n![Ridge Regression budget shown as an area](/assets/posts/linear_regression_geometry/ridge_lasso3.png)\n\nHowever, remember that though the coefficients can get larger than the original RSS=0 coefficients, we only care about the smaller coefficients since they are the ones that are highly likely to be met in the budget area. This is why Ridge Regression is also called a `shrinkage method`. The coefficients are shrunk to 0 to find the right combination that is within the budget. Since none of the points in RSS=1 line is within the budget, we need to continue sacrificing the RSS to find the right combination.\n\n![Ridge Regression budget shown as an area](/assets/posts/linear_regression_geometry/ridge_lasso4.png)\n\nAs we continue to sacrifice the RSS, the line of possible combinations of coefficients will be larger and larger. Eventually, we will find the right combination that is within the budget. This is the combination that minimizes the RSS while keeping the sum of the squares of the coefficients within the budget. Since both the area and our line are ellipses, the combination that is the best combination will be the point where both ellipses intersect to each other.\n\nThough we can sacrifice the RSS more and find more combinations that can fit within the boundaries, we have to remember that this is an optimization problem. Thus, the combination that is the best is the one that minimizes the RSS while keeping the sum of the squares of the coefficients within the budget. This is the combination that is the closest to the actual output values. This is in fact the point where the Ridge Regression model stops its exploration and outputs the coefficients.\n\n![Ridge Regression budget shown as an area](/assets/posts/linear_regression_geometry/ridge_lasso5.png)\n\n### Lasso Regression in a Geometric Perspective\n\nNow how is Lasso Regression expressed in the coordinates. First, we should remind ourselves how lasso regression is different from ridge regression. Lasso regression is also a shrinkage method, but it uses the sum of the absolute values of the coefficients as the penalty term. This is why Lasso Regression is also called L1 regularization.\n\nThe penalty term of Lasso Regression is expressed as below:\n\n$$\nMinimize \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n$$\n\nNow the penalty term is different from the Ridge Regression. Thus the budget area will not be shown as an ellipse but a rectangular. Everything else is the same. We need to sacrifice RSS untill there is a point that is within the budget area.\n\n![Ridge Regression budget shown as an area](/assets/posts/linear_regression_geometry/ridge_lasso6.png)\n\nIf you see the intersecting point, there is an interesting thing that happens. The Lasso Regression model can output a combination that has some of the coefficients to be 0. This is due to the shape of the budget area. In Ridge regression, the budget area was an ellipse, and thus it is hihgly unlikely for the intersecting points to end up on the axes. However, in Lasso Regression, the budget area is a rectangle, and thus it is highly likely for the intersecting points to end up on the axes. Due to this, Lasso Regression can output a combination that has some of the coefficients to be 0.\n\n## Conclusion\n\nThis is the end of the explanation of Ridge and Lasso Regression in a geometric perspective. I hope that this explanation was helpful in understanding how Ridge and Lasso Regression works. I would like to thank Professor [Soroush Saghafian](https://scholar.harvard.edu/saghafian) for helping me understand this topic!\n\n## Reference\n\n1. An Introduction to Stastistical Learning\n","categories":["ML","Data Science"],"thumbnail":"/assets/posts/linear_regression_geometry/ridge_lasso4.png","WIP":false,"data":{"title":"Linear Regression Model Selection (feat. Geomertry)","excerpt":"Linear Regression Model is a powerful tool for modeling multiple variables and their relationships to the targeted output variable. However, having many predictors can overcomplicate the model and output poor results. We will understand linear regression model selection methods (Ridge, Lasso) in a geometric perspective.","date":"2024-10-10","author":{"name":"Kim Dong Hun"},"keyword":"Model Selection","categories":["ML","Data Science"],"WIP":false,"thumbnail":"/assets/posts/linear_regression_geometry/ridge_lasso4.png"}},{"title":"VAE the basics","date":"2024-03-02","slug":"vae_overview","author":{"name":"Kim Dong Hun"},"excerpt":"Ever since I got interested in generative art, I felt a need to understand the basics of multiple neural networks. In many dissertations, I always found some articles that referenced VAE for generative purposes. In this post, I go through a brief overview of how VAE works","keyword":"VAE","content":"\n## What is VAE for?\n\nVariational Autoencoder (VAE) is known to be a more advanced Autoencoder (AE). The main difference between AE and VAE is that VAE has an additional statistical feature added to it in the middle. Both AE and VAE network inputs multi-dimensional data and outputs a multi-dimensional output that has a high resemblance to the inputted data. Thus, it means that AE and VAE are networks that try their best to output the same data that was initially inputted. How they train the parameters for making such output is through an encoder and decoder network, where the encoder tries to compress the multi-dimensional input into smaller data and make the decoder recreate(=reconstruct) the image with that compressed data. In the AI realm, this compressed data is called a vector in a latent space.\n\n![Encoder_Decoder](/assets/posts/vae_overview/encoder_decoder.png)\n\nThough AE and VAE have similar structures, their purposes differ. While AE's main focus is on the encoder, the VAE's main focus is on the decoder. This means that AE's purpose is to compress the multi-dimensional data into a compressed vector while VAE's purpose is to generate images. For instance, AE can be mainly used to create an image-based search system where every uploaded image goes through the encoder network to output a vector composed of numbers, which will be used to find similar images compared to a newly inputted image. VAE can be used for generative purposes such as generating a new image.\n\nIn this post, the main focus is to get a glimpse into how VAE works. As mentioned above, the feature that makes VAE more advanced compared to AE is that it has a statistical feature added in the middle of the network.\n\n![VAE](/assets/posts/vae_overview/vae.png)\n\n## The additional statistical feature in VAE\n\nSo I have previously said that there is an additional statistical feature in VAE. Why then is a probabilistic layer added to the latent space? VAE's purpose is to `reconstruct well`. Under the hood, VAE understands the inputted training samples as data with irregular and random noises. Thus a probabilistic layer on the latent space can filter out the noise that has not much to do with the [`inherent signal`](https://medium.com/tensorflow/variational-autoencoders-with-tensorflow-probability-layers-d06c658931b7) of the data.\n\nIn the VAE network, this probabilistic layer is implemented by extracting a `μ vector` and a `σ vector`, each representing the mean and standard deviation of the input data. In many practices, the VAE network is assumed to have a normal distribution, μ is 0 and σ is 1. Thus in the training process, while extracting two vectors, one for μ and one for σ, we must take measures to make the μ and σ follow the normal distribution.\n\nI first was curious whether it is possible to extract the μ and σ of a multi-dimensional input. Was there a mathematical function to get the μ and σ of any encoded data? However, after looking through some codes, it became evident that there does not exist a specific mathematical approach to calculating the μ and σ. In most networks, μ and σ are simply vectors that go through a linear layer in the encoder network. It is the loss function's job to make the linear layers to make the two vectors the μ and σ of a normal distribution. The element that makes the two vectors into the μ and σ of a normal distribution is the KL divergence loss element, which is something that is discussed in the later paragraphs.\n\n```python\nclass Encoder(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super(Encoder, self).__init__()\n\n        self.FC_input = nn.Linear(input_dim, hidden_dim)\n        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n\n        self.LeakyReLU = nn.LeakyReLU(0.2)\n\n        self.training = True\n\n    def forward(self, x):\n        h_       = self.LeakyReLU(self.FC_input(x))\n        h_       = self.LeakyReLU(self.FC_input2(h_))\n        mean     = self.FC_mean(h_)\n        log_var  = self.FC_var(h_)\n        return mean, log_var\n```\n\nAs you can see in the above code, there are not any mathematical measures added to make mean and variance vectors.\n\n## Sampling a latent space vector\n\nNow that we got a hang of the meaning of a probabilistic layer inside a VAE, it is now time to understand how the latent space vector is sampled. In short, how is a latent space vector, z, created through the extracted mean and variance vectors? The best method is to randomly choose a vector from the normal distribution represented by the mean and variance vectors. However, when a latent space vector, z, is created through such a process, there is no way the VAE model can update its parameters through backpropagation. The selection of a latent space vector through the statistical method is `not differentiable`.\n\nResearchers instead introduced a new approach to create a latent space vector out of the mean and variance vector. What they did was simply use arithmetic operations to create a latent space vector. $z=\\mu+\\sigma*\\epsilon$. $\\epsilon$ here is a randomly chosen vector in a normal distribution. This method is called the `Reparameterization Trick`. The math behind this trick is very complex and I will not go into detail about the math.\n\n## Math behind VAE\n\nFor a neural network to train its parameters, a clear purpose of the model should be set. As mentioned above, the main goal of the VAE is to `well reconstruct` the original input through the decoder. Thus, if we say the parameters of the decoder network is $\\theta$, our goal is to make the probability of x, original, maximum, $p_\\theta(x)$. We use our latent space vector, z to accomplish this, so the function should take z into consideration. If we change the function to incorporate z, it is $\\int p_\\theta(z)p_\\theta(x|z) \\, dz$. The reason $p_\\theta(x|z)$ appeared is because the combined probability of x and z can be represented as x|z. $\\frac{p_\\theta(x,z)}{p_\\theta(z)} = p_\\theta(x|z)$, which also means $p_\\theta(x,z)=p_\\theta(z)\\times p_\\theta(x|z)$. We know that $\\int p_\\theta(x,z) \\, dz = p_\\theta(x)$. In fact, $p_\\theta(x|z)$ is the representation of the decoder network of VAE.\n\nHowever, the problem is that the integral for $p_\\theta(x|z)$ cannot be calculated(=intractable). In the context of integral, having that $p_\\theta(x|z)$ means that we have to calculate the probability of x for all latent space vector z. Researchers thought that if $p_\\theta(x|z)$ cannot be directly, why not find the posterior distribution, distribution of latent variables given the observed data, $p_\\theta(z|x)$? In addition to escaping the burden of finding the $p\\theta(x|z)$, since the VAE's job is to generate outputs that are highly similar to the inputted data, a well-structured latent space can ensure continuity and completeness. `This is why a VAE necessitates the training of an encoder despite its main focus on the decoder`.\n\nUnfortunately, finding $p_\\theta(z|x)$ is also not an easy task since finding $p_\\theta(z|x)$ also necessitates a prior knowledge of $p_\\theta(x)$. $p_\\theta(z|x)=\\frac{p_\\theta(x|z)\\times p_\\theta(z)}{p_\\theta(x)}$. Thus we need to accept the fact that we cannot find the $p_\\theta(z|x)$ directly, but instead train an encoder network that is close to $p_\\theta(z|x)$. The encoder network will be represented as $q_\\phi(z|x)$.\n\nSo back to the original purpose of VAE. Our goal is to maximize the data likelihood, $p_\\theta(x)$. Let us add a log to the value to make our calculations simple. $\\log{p_\\theta(x^{(i)})}$. i is a value from 1 to N, and N represents the number of input data. To make the calculation simple we make the value as Expectation, $E_{z \\sim q_\\phi(z|x^{(i)})}[\\log p_\\theta(x^{(i)})]$. Here, z follows the distribution of our encoder($q_\\phi(z|x^{(i)})$). According to Bayes' rule, we can change the $p_\\theta(x^{(i)})$ into $\\log \\frac{p_\\theta(x^{(i)}|z)p_\\theta(z)}{p_\\theta(z|x^{(i)})}$. The following math is shown in the image below.\n\nBayes rule\n\n$$\np(z|x)=\\frac{p(x|z)\\times p(z)}{p(x)}\n$$\n\n![VAE](/assets/posts/vae_overview/vae_math.png)\n\nAs shown in the image, in the end, we end up with three terms we need to consider when maximizing $\\log{p_\\theta(x^{(i)})}$. Unfortunately, since $p_\\theta(z|x^{(i)})$ is intractable, the last KL term cannot be calculated. However, due to KL divergence outputting a result that is always \u003e=0, we can kindly disregard this term. The initial two terms combined are called the `Variational lower bound(=ELBO)` in a VAE, and our goal is to maximize the ELBO. The first term in ELBO is related to the decoder network and the second term in the ELBO is related to the encoder.\n\n### What we need to find\n\n$$\n\\{\\theta^*, \\phi^*\\} = \\operatorname{arg\\,max}_{\\theta, \\phi} \\sum_{i} \\left( \\mathbb{E}_{q_\\phi(z|x_i)} \\left[\\log p_\\theta(x_i|z)\\right] - \\mathrm{KL}\\left(q_\\phi(z|x_i) \\parallel p(z)\\right) \\right)\n$$\n\nSince loss functions are often expressed as minimizing the function. we can change the above expression into the loss function below\n\n$$\n\\{\\theta^*, \\phi^*\\} = \\operatorname{arg\\,min}_{\\theta, \\phi} \\sum_{i} \\left( -\\mathbb{E}_{q_\\phi(z|x_i)} \\left[\\log p_\\theta(x_i|z)\\right] + \\mathrm{KL}\\left(q_\\phi(z|x_i) \\parallel p(z)\\right) \\right)\n$$\n\nThe first term has to do with reconstruction error, where it outputs the error to how much the $x_i$ was wrongly reconstructed, and the second term has to do with the regularization error, where it outputs the encoder's deviation from the normal distribution $p(z)$ (remember that we initially supposed the distribution of latent space vector z to be a normal distribution)\n\n## Analyzing the loss function terms\n\nMinimizing the loss in regularization is done by the KLD operation for two distributions. Since the $p(z)$ was set to a normal distribution, the function is relatively simple (I will not discuss the specifics of how KLD is calculated)\n\n![VAE regularization calculation](/assets/posts/vae_overview/vae_regularization.png)\n\nMinimizing the reconstruction error is a little bit more complex due to the necessity to take probabilities into consideration. For this purpose, the researchers used a Monte Carlo technique to simplify the calculation of integral. Also, the L was set to 1 to make the calculation even simpler, resulting in a reconstruction loss calculation to $\\log(p_\\theta(x_i|z^{(i)}))$\n\n![VAE reconstruction calculation 1](/assets/posts/vae_overview/vae_reconstruction1.png)\n\nNow to further calculate $\\log(p_\\theta(x_i|z^{(i)}))$, the characteristic of the p should be determined. The probability can either follow a multivariate Bernoulli or a Gaussian distribution. In the case of bernoulli, $\\log(p_\\theta(x_i|z^{(i)}))$, the probabilities of each item in the $x_i$ should be multiplied. In logarithmic operation, multiplication can be considered as additions. Thus the below cross entropy can be calculated. By the way, $p_{i,j}$ is the decoder network output.\n\n![VAE Bernoulli](/assets/posts/vae_overview/vae_bernoulli.png)\n\n### References\n\n1. https://www.youtube.com/watch?v=GbCAwVVKaHY\u0026list=LL\u0026index=1\n2. https://medium.com/tensorflow/variational-autoencoders-with-tensorflow-probability-layers-d06c658931b7\n3. https://towardsdatascience.com/on-distribution-of-zs-in-vae-bb9a05b530c6\n4. https://medium.com/analytics-vidhya/variational-autoencoders-explained-bce87e31e43e\n5. https://sassafras13.github.io/ReparamTrick/\n6. http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf\n7. https://jaan.io/what-is-variational-autoencoder-vae-tutorial/\n","categories":["AI"],"thumbnail":"/assets/posts/vae_overview/vae.png","WIP":false,"data":{"title":"VAE the basics","excerpt":"Ever since I got interested in generative art, I felt a need to understand the basics of multiple neural networks. In many dissertations, I always found some articles that referenced VAE for generative purposes. In this post, I go through a brief overview of how VAE works","date":"2024-03-02","author":{"name":"Kim Dong Hun"},"keyword":"VAE","categories":["AI"],"WIP":false,"thumbnail":"/assets/posts/vae_overview/vae.png"}},{"title":"Sky Palette Project: 2. Sky Images are Different","date":"2023-08-20","slug":"sky_palette_2","author":{"name":"Kim Dong Hun"},"excerpt":"Continuing on the sky palette project, I started to go deeper into how I can best capture the colors in the sky. I tested with various algorithms, but nothing seemed to work really well. Different from other images, the sky had small details in the image that made the color very comprehensive. If I could capture the very details, I thought the there would be progress","keyword":"vision","content":"\nThis post is a continuation of the [Sky Palette Project 1](./sky_palette_1).\n\n## PCA Decomposition of the sky image\n\n![2D PCA Decomposition](/assets/posts/sky_palette/pca2.png)\n\nContinuing where I took off last night, I started to go deeper into the analysis methods for the sky image. Last time, I tried with K-means clustering. However, since K-means clustering was simple average of the pixel rgb colors, I thought it was not the best way to capture the expressiveness of the sky image.\n\nI first started with analyzing the PCA components of the image. They returned some interesting results, where I could see a beautiful gradient of the sky image. I wanted to extract those very colors that could make the gradient.\n\n![1D PCA Decomposition](/assets/posts/sky_palette/pca1.png)\n\nI tried to express the colors in one dimension too. They were not as beautiful as the 2D PCA, but I could see some gradient in the colors. I thought that if I could extract the colors in the gradient, I could make a beautiful palette.\n\n![PCA extraction for 5 colors](/assets/posts/sky_palette/pca_extracted_5.jpg)\n\nI did make a 5 color palette from the pca components. The colors seemed to represent the important colors in the sky image, however, there was a slight lack in perfection. For this particular sky image, I especially liked the red and purple gradient part, but this palette did not capture that part. However, according to the PCA analysis of the colors, the purple color and the blue color were considered to be very similar. Thus, it was difficult to capture both the dark blue and dark purple colors in the palette.\n\n## SNIC segmentation of the sky image\n\nAfter much contemplation, I decided to try with another method. While I was staring at some sky images, I discovered that sky images had a positional difference in the colors. Since the sky image is basically a gradient, the colors in the sky image are different depending on the position of the image. I thought that if I could capture the positional difference, I could make a better palette.\n\n![SNIC segmentation of the sky image](/assets/posts/sky_palette/segmented_image.jpg)\n\nI started segmenting the sky image with a [SLIC algorithm](https://darshita1405.medium.com/superpixels-and-slic-6b2d8a6e4f08). The SLIC algorithm is a superpixel segmentation algorithm that segments the image into regions of similar colors. I thought that if I could segment the sky image into regions of similar colors, I could capture the positional difference in the sky image. I used the SLIC algorithm from the scikit-image library.\n\nAfter segmenting the image, I extracted the colors from each segment. However, during the process, I came across with a question. \"What if the sky image is not vertically aligned?\" I thought that if the sky image was not vertically aligned, the colors would be different depending on the position of the image. Thus, I started to find ways to align the sky image vertically.\n\n## Image Rotation with LAB Deconstruction\n\n![LAB Deconstruction](/assets/posts/sky_palette/lab_deconstruction.png)\n\nAfter much research, I found that LAB deconstruction is best for differentiating colors. So I first changed the rgb image into a lab image. After the deconstruction, I found that there were clear distinctions in color areas. Using a convolutional neural network, I calculated the gradients of each pixels. Then, I calculated the mean of the gradients to find the best rotation angle.\n\nThe rotation seemed to work. However, I soon found out that this was not applicable to all sky images. The biggest problem occurred when there were clouds. Since LAB takes lightness into its consideration, the clouds had a clear distinction with the atmosphere background color. Thus the rotation angle did not seem to work when there were many clouds. Thus I decided to forgo the attempt to align the sky image.\n\nI started to go deeper into color segmentation using SNIC. Assuming that the sky image has a vertical color gradient, I grouped the segments according to the vertical position of the segment. Then, I extracted the colors from each group.\n\n## Result Comparison\n\n![Kmeans \u0026 SNIC compare](/assets/posts/sky_palette/kmeans_snic_compare.png)\n\nThis is the result comparison of Kmeans (the algorithm I used in the last try) and SNIC. Now that the positional difference was considered, the colors seem to be more visually pleasing. However, if you see closely, you can see that Kmeans actually captured the representative colors better. While, the SNIC-produced colors seem to be more in harmony, there are some colors that SNIC could not catch while Kmeans could catch. Plus, SNIC works only when the sky image is vertically aligned.\n\nAfter seeing the results, I started to think that maybe I should not segment the sky image. Though positional information would be great, the positional information cannot be applied to all sky images. Moreover, positional information may lead to ignoring the impressive color parts in the sky image. For instance, if there is more blue in the sky image, most of the colors in the palette would be blue. I needed to think of another way to capture the colors in the sky image.\n\n## Solutions?\n\nAfter much thought, I started to see the problem from a different perspective. I was consumed with the idea of preserving the spatial information of the image, but it seemed useless. I looked at the 2D PCA decomposition again. If only I could capture the colors in the 2D PCA decomposition, I could make a beautiful palette.\n\n![Radial Selection of PCA](/assets/posts/sky_palette/radial_selection_pca.png)\n\nThen, I suddenly thought of a new way to group the images in the PCA. **Why don't I group the colors in the PCA radially?** Starting from the middle of the PCA, I could group the colors in the PCA radially. This would be great for capturing anomalies since the anomalies would be in the outer part of the PCA. I will try this method next time!\n\nYou can see my code for this project [here](https://github.com/hunkim98/sky-palette).\n","categories":["project","AI","color"],"thumbnail":"/assets/posts/sky_palette/segmented_image.jpg","WIP":false,"data":{"title":"Sky Palette Project: 2. Sky Images are Different","excerpt":"Continuing on the sky palette project, I started to go deeper into how I can best capture the colors in the sky. I tested with various algorithms, but nothing seemed to work really well. Different from other images, the sky had small details in the image that made the color very comprehensive. If I could capture the very details, I thought the there would be progress","date":"2023-08-20","author":{"name":"Kim Dong Hun"},"keyword":"vision","categories":["project","AI","color"],"WIP":false,"thumbnail":"/assets/posts/sky_palette/segmented_image.jpg"}},{"title":"Sky Palette Project: 1. The Beginning of the Journey","date":"2023-08-19","slug":"sky_palette_1","author":{"name":"Kim Dong Hun"},"excerpt":"Sky Palette Project is a project that I started to extract colors from the sky. A professor once told me that the sky is the best color palette that a designer can use. The sky has a natural color gradient that is harmonious. I wanted to extract the colors of the sky and make it into a color palette. To begin that journey I first created a sky capturer for capturing the image of the skies","keyword":"vision","content":"\n## The sky is the best color palette\n\n![Design Homework 2020](/assets/posts/sky_palette/design_hw.jpeg)\n\nIn a design basics class that I took in 2020, the professor gave students a homework to create a color palette out of their photos. The professor recommended us to take multiple photos that we preferred and extract the colors from them. Students were free to take new pictures if they wanted to. I did not take the class seriously, and I just used photos from my college life and extracted colors from them. After everyone submitted their homeworks, the professor told us one thing that I still remember to this day. She said, **When you have trouble finding the right colors to use in your design, look up to the sky and take a picture of it. The sky is the best color palette one can use because the color harmony is already there.** The sky is where light and darkness meet. The clouds add variance to the colors. The master of colors is right above your head, each day with a different palette.\n\nEver since I heard those words, I was always fascinated with the beauty of the sky. It is nature color untainted by human hands. Unfortunately, I never had a chance to use the colors of the sky. Design homeworks could be finished just by surfing through many reference images shared in the internet. I would glance at the sky when I was tired of my work. It was full of awe, but I was not ready to digest it.\n\nNow, transformed into a researcher interested in both Visual Communication Design and Computer Science, I began to experiment on the intersections of the two fields. While I was reading a dissertation on color palette recommendation and its application in design, the words of my design professor back in 2020 came back to me. In the dissertation, the color dataset for training the recommendation algorithm were extracted from visuals examined by design experts. I thought to myself, **\"Why do we need to rely on professionals for color recommendation? Can't the nature itself present us with the best color palette?\"** This is how I embarked on the journey of extracting colors from the sky.\n\n## Gathering data from the sky with a sky capturer\n\nTo extract colors from the sky, I first needed to capture the sky. I first approached this problem in a technological perspective. I created a web app that periodically captures images every interval.\n\n![Sky Capturer Web App](/assets/posts/sky_palette/sky_capturer.PNG)\n\nIn the web app, I prepared features where a user could set an interval time for capturing the sky image. I first thought of making the capture interval time static, but I realized that the sky would change its color rapidly when the sun is setting or rising. Thus, I connected the web app to an api that tells the sun set time and sun rise time according to the user's location. I made the web app change its capture interval time to 5 minutes when the current time is near the sun set or sun rise time. This was to capture the sky effectively when the sky is changing its color rapidly. The captured images were stored in a cloud storage.\n\n![Capturing Image at home](/assets/posts/sky_palette/capturing_image.jpg)\n\nI first tested the feasability of the web app in my home. I opened my web app in an iPad and started recording the sky. I set my iPad like the above image for one day. Thankfully, the iPad withstanded the heat of the sun. However, the captured image quality was poor. I could not fully capture the sky because there were buildings that were obstructing the view of the sky. Moreover, due to the window screen, the image was blurry. I needed to find a better place to capture the sky.\n\n![Captured Image Sample (Home)](/assets/posts/sky_palette/captured_at_home.jpeg)\n\n## Gathering sky image data from the internet\n\nI thought of capturing the image of the sky in another place. However, after much thought, I realized that I could search some images online. Though capturing images manually would be great, the first priority was to see if my sky palette project will work. Thus, I started surfing the web to find the adequate images for the sky palette project.\n\nI first searched for sky image datasets in google. Some datasets that were used for analyzing cloud patterns were available. Most datasets required a request to the owner of the dataset. Thus, I went on to request the dataset in multiple websites. I got some images from NUS (National University of Singapore). Vision \u0026 interaction group in NUS had gathered sky cloud images of Singapore for their [research](https://vintage.winklerbros.net/swimcat.html). I got the data from NUS, however, I soon found out that I could not use their image dataset for extracting colors. Most images were basically images of the clouds. Moreover, there seemed to have not much variance in the color of the sky. I needed to find another dataset.\n\n![SWIMCAT sample images](/assets/posts/sky_palette/swimcat.png)\n\nI kept searching google for image datasets. I founds some noteworthy datasets that were simply collected as a hobby. However, many of them had objects or artifacts other than the sky, which would make it difficult to extract the colors of the sky. I needed to find a dataset that was specifically made for the sky. I could go on and perform color segmentation to distinguish the sky from other objects, but that could be done after I had made a working prototype of the sky palette project. I needed to find a find dataset that I could use right away.\n\nAfter much surfing, I found an adequate dataset from a artist named [Eric Cahan](https://ericcahan.com/bio/). He was a photographer who had special interest in taking photos of beautiful skies. In his [Sky Series Selected Work](https://ericcahan.com/portfolio/sky-series/), there were some fabulous images that I could use to extract the color of the sky. Thus, I downloaded his images and started to extract the colors of the sky.\n\n## Color Extracting Algorithm Approach: K-Means Clustering\n\nI first approached the problem of extracting colors from the sky with a simple algorithm. I used K-Means clustering to cluster the colors of the sky. Many blogs explained that K-Means clustering is a simple algorithm that can be used to extract colors from an image. Thus, I first tried to use K-Means clustering to extract colors from the sky. I first tried to extract 5 colors from the sky.\n\n![Simple Kmeans Palette](/assets/posts/sky_palette/sky_palette_kmeans_initial.png)\n\nThe result was not too bad. But I felt it was not enough. I first tried extracting 5 colors. However, the 5 colors did not seem to harmonize well. What made it worse was that the 5 colors did not seem to represent the whole sky image. The sky image was beautiful and had many colors. Especially, the mix of red and blue was beautiful. However, due to Kmeans clustering method of taking the average of the colors, the represented colors were very dull.\n\nI thought that increasing the clusters would solve the problem. Though it did seem to improve, the colors were still dull. To me, when the cluster size was set to 15, the extracted colors seemed to represent the sky image well. I felt the need to tweak the Kmeans clustering algorithm to better extract the representative colors of the sky image. Also, in the hindsight, I felt that the ordering of the palette colors could have affected me to view the 5 clusters representation as a mispresentation of the sky image. Due to the random ordering of the colors, the colors seemed to not harmonized. If you see the sky image, you can see there is a gradual change in the color of the sky. However, once the images are seperated and presented discretly and color blocks, they look a bit awkward. I began to think that there is a need to order the colors as the sky image is ordered to lessen the awkwardness of the color blocks.\n\nI end this post here since my initial goals, gathering sky image data and going through basic kmeans algorithm approach, were sufficed. I will continue writing about this project in the next post. Anyone interested in the project can check out the [github repository](https://github.com/hunkim98/sky-palette)\n\nYou can read the next story in [Sky Palette Project (2)](./sky_palette_2)\n","categories":["project","AI","color"],"thumbnail":"/assets/posts/sky_palette/sky_palette_kmeans_initial.png","WIP":false,"data":{"title":"Sky Palette Project: 1. The Beginning of the Journey","excerpt":"Sky Palette Project is a project that I started to extract colors from the sky. A professor once told me that the sky is the best color palette that a designer can use. The sky has a natural color gradient that is harmonious. I wanted to extract the colors of the sky and make it into a color palette. To begin that journey I first created a sky capturer for capturing the image of the skies","date":"2023-08-19","author":{"name":"Kim Dong Hun"},"keyword":"vision","categories":["project","AI","color"],"WIP":false,"thumbnail":"/assets/posts/sky_palette/sky_palette_kmeans_initial.png"}},{"title":"Techstars Day2 ~ End - Building Networks, Defining Your Business","date":"2023-02-25","slug":"techstars2","author":{"name":"Kim Dong Hun"},"excerpt":"For a startup, you have nothing you can give to the other person when you want to reach out to them. However when you seek advice from the other person, the story gets a little different, and there will be a deep connection between you and the mentor. We also had practices on pitching ourselves. To pitch ourselves, we had to know the product that we were selling. The product of a company is not what the company actually sells. The customers who have evolved thanks to your product are your real products.","keyword":"startup","content":"\r\nI soon head back to Korea, and I thought that I might never write a post about the Techstars experience after I go to Korea. In this post, I would like to cram all the things I learned from Techstars since Day 2 to its final day.\r\n\r\n## Build network with mentors by asking advice from them\r\n\r\nOn Day 2, John Hill, a post-LinkedIn person, came to the seminar to give us a lecture of how to build networks and find people you need through LinkedIn. Before that, we had been lectured on discovering the 3Ws, Who (Who are you selling to?), What (What are the customers buying - NOT what you are selling!), Why (Why are the customers buying). I do not remember the exact details that were dicsussed in the lecture, but I do remember that we were going through the 3Ws because we had to be prepared to pitch ourselves effectively. In our short pitch (7 ~ 15 seconds), all the 3Ws had to be included to make our pitch clear and informantive.\r\n\r\nThe importance of preparing a clear pitch 3Ws is not limited to us for gaining attention from investors. It has importance because that clear pitch consisted of maybe 3 to 4 sentences is the phrase other people that know us can use to introduce to other people. Imagine we send an email about our business to another person. The person replies that she is not particularly interested but says that she will definitely introduce us to someone else that might be interested. Now she is on the her table writing an email about us to another person who she thinks might be interested in our business. What would happen if we did not provide her a clear informative pitch that she could simply copy and paste on her email? She would have a hard time trying to introduce our business in her own words making the whole process of trying to introducing us to another person cumbersome. She will be easily discouraged to introduce us to other people.\r\n\r\nThe seminar from a post-LinkedIn person's lecture on building network came after the lecture on the importance of creating an informative pitch and how to create a revenue formula. He told us how we could effectively use LinkedIn to easily reach out to someone we could ask guidance from. The one thing that he put great emphasis on while he taught us how we could do it was, \"If you are asking for money in the first meeting of an investor, you are losing\". When you are trying to approach a mentor, be it a investor or an expert, ASK FOR AN ADVICE. When you approach someone asking for an advice, everything shifts since once they have given you a piece of advice you have literally allowed them to get invested in you. Now they take part in your journey.\r\n\r\nHill also remined us that mentors are not paid. They genuinly have no reason to interact with you. However they will mentor you out of altruisum or out of interest in recent business trends. The motivation based on altruism is simply their want to give back what they have earned from their journey. The motivation based on interest in recent business trends is self-explanatory.\r\n\r\nMore to the HOW to use LinkedIn. Hill said that there are special tabs for organizations. Schools have 'alumni' tabs, and companies have 'life' tab or 'people' tab. In fact, these additional tabs especially for companies is actually some feature that companies have to pay for if they would like to have that in their profiles. These 'life' tab or 'people' tab is the best tab you can refer to if you want to know the language or culture of the company because they will definitely have that tab for a reason.\r\n\r\nLast but not least, Hill talked about the importance of watering your network everyday. He says that he personally strictly tightens his network to get a clear network of his first level communications. His directly connected people would be consisted of people he could directly contact with and grab a meeting with the other person that his directly connected person would be in direct connection. Also he recommended posting on LinkedIn since mentors are interested in your journey and they want to see it. Posting under 3 categories was what he also emphasized. His last words of the seminar was \"Build a network before you need it\".\r\n\r\n## Mario analogy for defining your business\r\n\r\nDay 2 ended with John Hill's talk about building networks, and I do not exactly remember what other lectures there wre after Day 2. Later on it was mostly a continuation of meeting mentors and so on. It was till Day 3 that we were lectured, and from then on, each team individually had to meet with mentors and so on. There is not much that I remember from the lectures from then on, but there is one analogy that I remember to this day, and that is what I would like to share in this post.\r\n\r\n![mario analogy](/assets/posts/techstars2/mario.png)\r\n\r\nThe above image is brought from UserOnBoard. This is analogy that Kerty used while talking about how we should pitch ourselves. To anyone familiar to Mario game, when the player eats a mushroom, the player evolves (gets bigger). Kurty taught us the core information that we needed to include when we were to pitch our business in 30 seconds. The pitch had to contain who the customers are, what problem do we sove, what the product is, why should you (investors) care, and optionally a breif founder market fit (ex. \"I previously worked in NASA as an engineer\" would be powerful if your business ahd to do with space engineering). The template provided was, \"There are ~ problems these days. Our solution product is ~. We provide solution to ~, SO THAT OUR CUSTOMERS CAN ~\". The \"SO THAT\" part is the most important part. Like explained in the mario image, what the company is selling is not actually the product itself, but the customers who have evolved after using our product.\r\n\r\nThis mario anoalgy is the reason why I wanted to write this post. It delivers a message that is crucial to a startup. A product is nothing without customers. In fact, in Techstars Day 2, we were constantly reminded the importance of gathering customer testimonials and making the practice of meeting customers one by one daily a habit. The importance of having a customer is also emphasized in a book I am reading now, `Rework`. Seeing customers enjoy your product acts as an important motivation to all team members. Remember, it is not fancy technology or product that you should focus on and put it upfront when you are selling to others. Your real product are the customers that have evolved thanks to your product.\r\n","categories":["diary","startup"],"thumbnail":"/assets/posts/techstars2/mario.png","WIP":false,"data":{"title":"Techstars Day2 ~ End - Building Networks, Defining Your Business","excerpt":"For a startup, you have nothing you can give to the other person when you want to reach out to them. However when you seek advice from the other person, the story gets a little different, and there will be a deep connection between you and the mentor. We also had practices on pitching ourselves. To pitch ourselves, we had to know the product that we were selling. The product of a company is not what the company actually sells. The customers who have evolved thanks to your product are your real products.","date":"2023-02-25","author":{"name":"Kim Dong Hun"},"keyword":"startup","categories":["diary","startup"],"WIP":false,"thumbnail":"/assets/posts/techstars2/mario.png"}},{"title":"Techstars Day1 - Spare just 1 hour and you can build trust within members","date":"2023-01-19","slug":"techstars1","author":{"name":"Kim Dong Hun"},"excerpt":"No matter what kind of work the team members are doing, if the team has decided to embrace a new team member, everyone should take a leave for their work at least for 1 hour to openly talk with the newcomer and open themselves up to vulnerability. No excuses whatsoever. The newcomer is a priceless and thankful person to have decided to join the risky startup journey together with the team, and thus, the team should pay their utmost respect to the newcomer.","keyword":"startup","content":"\n## Spare just 1 hour and you can build trust within members\n\nI am currently in Boston attending an event hosted by techstars. My brother started a startup with a colleauge in UC Berkely last year, and I came to the States to help him build his products with my software engineering skills. His startup was chosen as one of the twelve startups selected by Techstars for the evennt. Twelve companies doing their business in the crypto world gathered in Boston to get mentored by Techstars.\n\nOn the first day of the Techstars event (2023-01-19), all attendants sat in a room to listen to a lecture on teamwork. I have never attended to any MBA classes, but one could say the whole lecture was quite similar to classes provided in MBA programs. We were taught about some theories and strategies to manage a team. But I believe this lecture had more vitality than a normal MBA class in that all participants were CEOs of their startups and thus were eager to get the most out of the lecture.\n\nAt first, the whole lecture seemed boring. As a student who had listened to business management classes back in Seoul National University, all the things that were taught by the lecturer seemed too obvious. The main topic was about functions of a team, what differentiates a good team and a bad team. However, as the lecture progressed, I felt there were some parts worth notetaking.\n\n![Boston Techstars](/assets/posts/techstars1/meeting.png)\n\n### Develop deep relations when a newcomer joins\n\nIn the first part of the lecture, the lecturer asked each one of us to answer to three questions.\n\n1. Where did you grow up?\n2. How many siblings do you have? (Are you the youngest or the oldest?)\n3. Did you have any unique challenges during your childhood. And if so, how did that challenge affect you in your life.\n\nThe first two questions are just simple questions, but the last one is something that requires some thought and some time to think of. Taking turns, people talked about their unique challenges. Some talked about how their unique challenges made eventually made them do a startup, some just talked about their difficult childhood, and some talked about their coutnry's poor situation. Each person took a significant amount of time talking of their own stories.\n\nIt took approximately one hour for all the 30 people to share all their stories. After everyone spoke, the lecturer asked us if we felt more acquainted with one another. And it sure did. He was lecturing the importance of opening up your vulnerability to your team members, and he told us that by going through this process, team workers can get to know each other better and be reliable to each other. It just took 1 hour for 30 people to answer the questions. How long will it take for a team of 10 people to go through this process? Just sparing only a few minutes will be enough to make everyone feel deeply connected to each other.\n\nThis was the part I realized the importance of having an off-site occasion whenever a newcomer joins a group. No matter what kind of work the team members are doing, if the team has decided to embrace a new team member, everyone should take a leave for their work at least for 1 hour to openly talk with the newcomer and open themselves up to vulnerability. No excuses whatsoever. The newcomer is a priceless and thankful person to have decided to join the risky startup journey together with the team, and thus, the team should pay their utmost respect to the newcomer. Remove all the walls you may have in normal situations and open yourself to the newcomer. The best way to form the atmostphere of being connected is to ask each participant about their childhood difficulties.\n\n### Boring team meetings is a meeting where there are no conflicts\n\nNext, we were lectured about conflicts. Some of the lecture notes were obvios, but what struck me was the sentence, \"Boring team meetings is a meeting where there are no conflicts.\" The lecturer also added that if a team meeting ended without having any conflicts is a bad sign. The fact that there are some conflicts regarding their project (personal conflicts are disregarded here).\n\nI personally reflected on myself a lot on this subject. When I am in a situation of managing a team, I tend to be some sort of a manager-who-knows-all (As a type of person who plans everything, I plan everything before doing some project). When somebody objects to what I have planned for a project, it is quite difficult for me to discuss that matter in a leisurely manner. To other team members, I believe I might seem as though I am a stubborn boss when having a conflict where I am in charge of the project.\n\nThe lecturer commented about the importance of creating an environment where an employee can speak up and raise objections. If your attitude makes the team have a hard time raising a healthy conflict in the first place, it is important to contain yourself for the team. The lecturer talked about one CEO who practiced to contain himself whenever an employee raises an objection. The reason he practices it is because he knows well that once he takes immediate respondence to that opposite opinion, the person who raised an objection will fall down immediately and seldomly raise any objections in the future. The lecturer commented that to remedy the situtation, the CEO came up with an idea to always put a person who would always replies with a 'thank you for voicing your opinion' to the person who raised the objection in a meeting. When I happen to be leading a team, I believe I should also think of my own ways to make myself and the team openly object to any ideas they think are wrong or could be done better.\n\nDelving in to the topic about reacting to conflicts, the lecturer said that how you react to conflicts are predominantly learned from your caregivers. No matter what environment you were reared from, how you deal with conflicts does not change much from how you were learned to deal with conflicts from your caregivers. How you react to conflicts with other people has to do with how your parents fight each other and later patch up.\n\nThis also made me reflect upon myself again. Mostly my mother would be quiet when there is a fight between her and father. Similar to her, when I am being the one being scolded especially in personal conflicts, I tend to stay quiet. I cannot come up with any sentences to respond to that scold unless it is something utterly nonsense. When emotions rush, my sentence get loose. When some work does not seem to go well, I feel somewhat irritated and add emotion to my words. I believe this is somewhat similar to how my mother expresses anger in some occasions. She sometimes express her discomfort in my father's actions with emotion full of irritation.\n\n### Beware Triangulation.\n\n![Triangulation](/assets/posts/techstars1/triangulation.png)\n\nThe last memorable topic that the lecturer taught about was 'Triangulation'. 'Triangulation' is a term the lecturer himself coined up. To briefly explain what it is, it is a situation where B tells C the problems he/she has with A instead of having direct conversations with A. The lecturer said that this is one of the signal that a team is not operating well. It seems very obvious, but coming to think of it, it happens rather often in many cases in life, even mine also. The main reason why Triangulation is harmful to a startup especially is because it means that team members are avoiding conflicts. They are just being bystanders to a dying company. This can also lead to decreased commitment among team members since avoiding direct conflict with another person means that people are trying separate in groups. What can be worse than a small group of people in a startup dividing into groups?\n\nThese were the things I wanted to share for the first day of Techstars. It was a semi-MBA class but I believe it was worth it. I actually learned a lot and had some time to reflect on myself.\n","categories":["diary","startup"],"thumbnail":"/assets/posts/techstars1/meeting.png","WIP":false,"data":{"title":"Techstars Day1 - Spare just 1 hour and you can build trust within members","excerpt":"No matter what kind of work the team members are doing, if the team has decided to embrace a new team member, everyone should take a leave for their work at least for 1 hour to openly talk with the newcomer and open themselves up to vulnerability. No excuses whatsoever. The newcomer is a priceless and thankful person to have decided to join the risky startup journey together with the team, and thus, the team should pay their utmost respect to the newcomer.","date":"2023-01-19","author":{"name":"Kim Dong Hun"},"keyword":"startup","categories":["diary","startup"],"WIP":false,"thumbnail":"/assets/posts/techstars1/meeting.png"}}],"postCategories":["Geometry","Mathematics","ML","Data Science","AI","project","color","diary","startup"],"allProjects":[{"title":"Visualizing a Nation's Blood Donation and Demand","date":"2024-12-21","slug":"korea_blood_donation","excerpt":"There have been multiple articles explaining the blood donation system worldwide is suffering from a lack of blood donation. To understand whether such a problem exists, this data visualization project targeted the blood donation system of South Korea and visualized its blood donation status.","keyword":"Data Visualization","content":"\n[Visit Website →](https://hunkim98.github.io/korea-blood-donation/)\n\n![Demo](/assets/project/korea_blood_donation/thumbnail.gif)\n\nMany report that the blood donation rate is decreasing worldwide. In January 2024, Red Cross announced that the current status of blood donation is a emergency blood shortage, explaining that the number of U.S. blood donors hit all-time low for the past 20 years. To confirm whether such a problem exists, I decided to target a country that has a centralized blood donation system, South Korea, and visualize its blood donation status from 2000 to 2022.\n\nThe data was collected from KOSIS(KOrean Statistical Information Service). We gathered the number of blood donors, the number of blood donation, and the number of blood donation centers. We defined data that was related to donation as supply. To gather data related to demand, we collected additional data from KOSIS such as the number of patients, the number of surgeries, and the number of blood transfusions. By extrapolating how much blood is used for each surgeries and blood transfusions, we were able to estimate the demand for blood. Lastly, we also collected data related to what events the Red Cross held to promote blood donation in the hopes of finding correlation between the promotional events and the number of blood donors.\n\n![Data Heatmap](/assets/project/korea_blood_donation/heatmap.png)\n\nWith all our data, we decided to create an effective visual representation of the blood donation supply, blood usage demand, and the promotional events. Since we found that there is an uneven distribution of blood donation frequencies depending on the month, we decided to find a way to effectively visualize the ratio of blood donation and blood usage rather than their absolute values. We ended up using a balancer-looking shape to represent the ratio of blood donation and blood usage. The left thick line represented the blood donation, and the right colorless thin line represented the blood usage. The thicker the left area, the more blood donation there was compared to the blood usage. We used color and angle to represent how much the ratio is off.\n\n![Data Heatmap](/assets/project/korea_blood_donation/tokenization.png)\n\nThrough the visualization we could find that South Korea especially has a healthy ratio of blood donation and blood usage especially near May, and this seems to be due to the summer season and the frequency of promotional events held by the Red Cross. However, the ratio of blood donation and blood usage is not as healthy in the winter season. Also, after March 2020, the time when COVID-19 started to spread in Korea, the ratio of blood donation and blood usage has decreased significantly. From this, we can know that the COVID-19 pandemic has had a significant impact on the blood donation system in South Korea.","categories":["Data Visualization","Policy"],"thumbnail":"/assets/project/korea_blood_donation/thumbnail.gif","WIP":false,"data":{"title":"Visualizing a Nation's Blood Donation and Demand","excerpt":"There have been multiple articles explaining the blood donation system worldwide is suffering from a lack of blood donation. To understand whether such a problem exists, this data visualization project targeted the blood donation system of South Korea and visualized its blood donation status.","date":"2024-12-21","author":{"name":"Kim Dong Hun"},"keyword":"Data Visualization","categories":["Data Visualization","Policy"],"WIP":false,"thumbnail":"/assets/project/korea_blood_donation/thumbnail.gif"}},{"title":"Dotting GenAI: Pixel Art Editor With Generative AI","date":"2023-04-11","slug":"dotting_genai","excerpt":"Dotting GenAI is an experimental project that aims to incorporate generative AI into the realm of pixel art. Alongside software development, a user research was conducted in order to gain insights into how experienced designers perceive the collaborative potential with Generative AI.","keyword":"Generative AI","content":"\n[Hackathon Presentation Video →](https://www.youtube.com/watch?v=nKYJNuTxfTs)\n\n![Demo](/assets/project/dotting_genai/demo.gif)\n\nGenerative AI is a hot topic discussed everywhere. In the hopes of decreasing the barrier for creating images, many researchers are publishing better image generation models everyday. However, one problem exists in current Generative AI services: You cannot modify the generated images once they are generated.\n\nFor generative AI to be usable, the user should be able to modify the generated image. The modification can be as simple as changing the color of the generated image, or as complex as changing the shape of the generated image. For enabling modifications, the generative AI services should be integrated into an editor that provides additional tools that can be used to modify the generated images. However, there are no definitive software design principles on how to merge the two distinct experiences: generating and editing.\n\nTo test the methods of integrating Generative AI into editors, I created a pixel art editor with Generative AI named Dotting GenAI. The project was also a contestant for the 2023 Hackathon hosted by the a Korean Venture Capital, Primer. Dotting GenAI was awarded as one of the top 16 projects in the hackathon. It was created with an opensource project that I have started to provide React developers a pixel art editor, `Dotting`.\n\n![Editor screen](/assets/project/dotting_genai/home.png)\n\nDotting GenAI allows users to interact with the generative AI through a chat interface positioned on the right of the screen. The user can ask the generative AI to generate an image, and the generative AI will generate an image based on the user's request. The generated images will return as replies in the chat log. The user can place the generated image on the pixel art canvas and then modify the generated image with the editor tools provided by Dotting GenAI.\n\nAfter the development of the service, I conducted a user test on design experts to figure out 1) whether my software design approach was effective, and 2) whether the generative AI was useful for the design experts. The user test was conducted with 12 design experts, and the results showed some interesting insights.\n\nMost users thought the experience of being able to modify the generated images directly in the editor as comfortable. However, they had many worries about using generative AI in their work. Some reported that although generated images do encourage untried styles, they become a burden when the user wants to create a specific image they have in mind. Some also reported that they rather wanted a generative AI that learns from their own styles, rather than some random generated images based on prompts. People more interested in the study may download the \u003ca target=\"_blank\" href=\"/assets/project/dotting_genai/dissertation.pdf\"\u003ePDF file\u003c/a\u003e for more details.\n\n![Presentation in Primer Hackathon](/assets/project/dotting_genai/presentation.png)\n","categories":["hackathon","AI","editor"],"thumbnail":"/assets/project/dotting_genai/demo.gif","WIP":false,"data":{"title":"Dotting GenAI: Pixel Art Editor With Generative AI","excerpt":"Dotting GenAI is an experimental project that aims to incorporate generative AI into the realm of pixel art. Alongside software development, a user research was conducted in order to gain insights into how experienced designers perceive the collaborative potential with Generative AI.","startDate":"2023-03-22","date":"2023-04-11","author":{"name":"Kim Dong Hun"},"keyword":"Generative AI","categories":["hackathon","AI","editor"],"thumbnail":"/assets/project/dotting_genai/demo.gif","WIP":false}},{"title":"Cryptogalaxy: Visualizing cypto markets","date":"2022-12-11","slug":"cryptogalaxy","excerpt":"CryptoGalaxy is an attempt that visualizes the crypto market. It borrows its concept from the ‘space’ and connects galaxy components with several crypto market indicators. The main purpose of the project was to aid viewers understand the crypto market with visual components.","keyword":"crypto","content":"\n[Visit Website →](https://hunkim98.github.io/cryptogalaxy/)\n\n![CryptoGalaxy Logo](/assets/project/cryptogalaxy/logo.png)\n\n![Demo](/assets/project/cryptogalaxy/demo.gif)\n\nCryptoGalaxy attempts to visualize the Crypto Market in a fun and interactive way.\n\nTraditionally, Markets are represented as a table of numbers, full of jargon and hard to understand. CryptoGalaxy aims to change that by providing a fun and interactive way to visualize the Crypto Market.\n\nThe Crypto Market is a market where expert traders and normal traders participate in. Each participant hold to their own strategies and make decisions. To make a strategy, one must understand all sorts of indicators such as Moving Average, RSI, and so on. Unfortunately since all indicators are comprised of numbers and graph, it is very difficult for a newcomer to get familiar with trading.\n\n![Crypto Planets Orbiting BTC Sun](/assets/project/cryptogalaxy/screen.png)\n\nCryptoGalaxy is an attempt that visualizes the crypto market. It borrows its concept from the ‘space’ and connects galaxy components with several crypto market indicators. The main purpose of the project was to aid viewers understand the crypto market with visual components.\n\nThe conduits that convey information in CryptoGalaxy are the sun, planet, and spaceships. The Sun represents Bitcoin(BTC), and planets represent other coins such as Ethereum(ETH), Dogecoin(DOGE). The sentiments regarding a specific coin(planet) was shown with the in-and-outs of the spaceships.\n\nThe indicators used for visualization were Moving Average Increase Rate, Market Capital, Relative Strength Index(RSI), Money Flow Index(MFI), and Correlation Coefficient of the trend similarity between BTC and a specific coin. Since Moving Average Increase Rate is often related to how lucrative a coin is, the indicator was connected to the brightness of the sun(BTC), and the orbit speed of the planet(other coins) respectively. The Market Capital was connected to size of a planet. The RSI was connected to how spaceships come in to a planet and come out of the planet. The MFI was connected to the ice age degree of the planet. The Correlation Coefficient of the trend similarity between BTC and a specific coin was connected to how far a planet is from the sun.\n\n![Installation Viewed from Side](/assets/project/cryptogalaxy/front.jpg)\n","categories":["data-visualization"],"thumbnail":"/assets/project/cryptogalaxy/demo.gif","WIP":false,"data":{"title":"Cryptogalaxy: Visualizing cypto markets","excerpt":"CryptoGalaxy is an attempt that visualizes the crypto market. It borrows its concept from the ‘space’ and connects galaxy components with several crypto market indicators. The main purpose of the project was to aid viewers understand the crypto market with visual components.","startDate":"2022-09-01","date":"2022-12-11","author":{"name":"Kim Dong Hun"},"keyword":"crypto","categories":["data-visualization"],"thumbnail":"/assets/project/cryptogalaxy/demo.gif","WIP":false}},{"title":"Webnovelr: Text editor service for web novelers","date":"2022-12-11","slug":"webnovelr","excerpt":"Webnovelr is a text editor prototype I designed for web novelers. To figure out the necessary features of the service, I conducted an interview on experts and created a prototype website for the editor based the research","keyword":"webnovel","content":"\n[View Website →](https://hunkim98.github.io/webnovelr/)\n\nThe webnovel ecosystem is comprised of various writers and readers. However, despite the ecosystem being a media where many people come and go, the truth is that there are many visual aspects lacking considerations of the participants. Webnovelr is a project that attempts to better the ecosystem of webnovels.\n\nWebnovels are usually served by only a few platforms in Korea. The representative platforms are Kakao Page and Munpia. Webnovel writers send their literature to these platforms. Each platform have different layouts, thus showing the same literature in a different way. Writers name these differences as ‘Jopan’, which is known to affect how readers read the literature.\n\nTo writers, ‘the reader’s speed of reading’ is as important as the originality of the webnovel. If a tense scene does not fit in one page but continues on to the next page, there is a possibility where readers can feel bored of the webnovel. Despite these concerns, there exists no editor that allows webnovel writers to keep track of the reader’s speed of writing. To make the writing experience easier for webnovel writers, Webnovelr presents a ‘Jopan’ based editor.\n\n![Home Screen](/assets/project/webnovelr/home.png)\n\n![Text Edit Page](/assets/project/webnovelr/edit.png)\n\n![Keyword Search UI](/assets/project/webnovelr/keyword_search.png)\n\n![Character Edit Page](/assets/project/webnovelr/character_list.png)\n\n![Exhibition at Seoul National University](/assets/project/webnovelr/installation.jpg)\n","categories":["data-visualization","editor"],"thumbnail":"/assets/project/webnovelr/screen.png","WIP":false,"data":{"title":"Webnovelr: Text editor service for web novelers","excerpt":"Webnovelr is a text editor prototype I designed for web novelers. To figure out the necessary features of the service, I conducted an interview on experts and created a prototype website for the editor based the research","startDate":"2022-09-01","date":"2022-12-11","author":{"name":"Kim Dong Hun"},"keyword":"webnovel","categories":["data-visualization","editor"],"thumbnail":"/assets/project/webnovelr/screen.png","WIP":false}},{"title":"LinkLink: Visualize my social networks","date":"2022-12-08","slug":"linklink","excerpt":"LinkLink is a social networking application that is focused on reaching out to the acquaintances of my friends. By providing a visualized social network graph consisted of my friends and the friends of my friends, one can find their suitable companions easily.","keyword":"social network","content":"\n[Project Wiki →](https://github.com/swsnu/swppfall2022-team9/wiki/Requirements-and-Specification)\n\n![Demo](/assets/project/linklink/demo.gif)\n\nWhen people start a startup, they start by finding suitable people to work with. In most cases, they reach out to their friends and ask them to join the startup. If the friends say that they are unavailable, then the person ask them to introduce their friends. This is because getting referrals from trusted relationships is the best way one can gather suitable companions. This process is very common in the startup world, and I have experienced this process myself. However, the problem is that this process is very inefficient. You have to meet each of your friends to get information about their acquaintances, and then ask your friends to introduce you some friends you think might fit with you.\n\nWould this whole process be simplified if there was a service that showed the qualities of the friends my friends have? If that was the case, I can directly contact my friend for a specific person that I have interest in. That is why I came up with LinkLink, a social networking application just for that purpose. There are many social networking applications out there such as LinkedIn. However the problem is that their purpose is too general, and thus not suitable for specific purposes. LinkLink focuses on the \"friend of a friend\" process, where one can view the qualities of the friends of their friends, and then ask the intermediary friend to introduce him or her to oneself.\n\n![Friend Network Visualization](/assets/project/linklink/home.png)\n\nLinkedIn previously did a [similar project](https://blog.linkedin.com/2011/01/24/linkedin-inmaps) in 2011. It visualized all the networks of the acquaintances you had. However, the problem was that the visualization was too complex, and thus not very useful. There was too much information in the visualized network resulting in an ununderstandable mess. LinkLink solved this visual mess by putting a limit to how many direct friend connections one can have (one can only have a maximum of 15 direct friends). This benefits the service in two ways: 1) The service is more reliable since one has to carefully choose who to connect with, resulting in a more reliable network, 2) The user is able to understand the network better since there are less connections to look at.\n\n![Personal Profile UI](/assets/project/linklink/profile.png)\n\n![Evaluate Friends](/assets/project/linklink/evaluate.png)\n\nSince the main purpose of the application was to provide users with reliable networks, our team devised evaluation features for users to evaluate their directly connected friends. This could allow users to not only judge the fitness of another person based on their skillsets but also their character. When one finally decides they want to connect with that person, they can chat with the person directly or ask their intermediary friend to introduce them to the person.\n\n![Chat](/assets/project/linklink/chat.png)\n","categories":["data-visualization","social-network"],"thumbnail":"/assets/project/linklink/demo.gif","WIP":false,"data":{"title":"LinkLink: Visualize my social networks","excerpt":"LinkLink is a social networking application that is focused on reaching out to the acquaintances of my friends. By providing a visualized social network graph consisted of my friends and the friends of my friends, one can find their suitable companions easily.","startDate":"2022-09-01","date":"2022-12-08","author":{"name":"Kim Dong Hun"},"keyword":"social network","categories":["data-visualization","social-network"],"thumbnail":"/assets/project/linklink/demo.gif","WIP":false}},{"title":"Toonie: Real-time Collaborative image review editor","date":"2022-08-19","slug":"toonie","excerpt":"Toonie is a real-time CRDT-based collaborative image review editor where users can review on images uploaded to the service together. I developed software design appraoches for optimizing collaborative whiteboards.","keyword":"collaboration","content":"\nCollaborative editing is a rising star in software development. Now, people not only want smart editors, but they also want an editor that allows real-time collaboration with other people. Collaborating with other people is a great way to increase productivity and get feedback on your work. The COVID-19 pandemic accelerated the familiarity of collaborative editing, and now, many people are using collaborative editors in their daily lives.\n\nHowever, implementing a multiplayer aspect into an editor is never an easy task. First, one should have basic knowledge of how to manage multiple actions from multiple agents. What happens if two people try to edit the same part of the document at the same time? What happens if one person deletes a part of the document while the other person is editing the same part? These are the questions that one should be able to answer when they want to implement a collaborative editor. Second, one should have to decide what features shall be communcated between users, and how their actions do not cause overhead. Relaying data between users can be costly and unreliable in some cases, so one should carefully design the data flow between users in a way that it does not harm user experience.\n\nToonie is a real-time CRDT-based collaborative image review editor that I created to research optimization techniques for implementing CRDT mechanisms into whiteboards. In Toonie, users can review on images uploaded to the service together by sharing a URL to another person. The reason I chose an image review domain specifically is because most collaborative editors out there are for general purposes. Thus, I wanted to create a more narrow focused whiteboard where designers and marketers could review on images together by drawing sketches on the images in an online meeting setting.\n\n![Editing Scene](/assets/project/toonie/edit.png)\n\nToonie wass built on top of the CRDT-based collaborative SDK called [Yorkie](https://github.com/yorkie-team/yorkie) which is an opensource document store for building collaborative applications created by Naver Alto TF. While contributing to the opensource project, I created Toonie for demonstrating the capabilities of Yorkie.\n\nWhen creating a multiplayer whiteboard, one should distinguish between actions that need its commit order to be preserved and actions that do not need its commit order to be preserved. This is because in multiplayer operations, actions that needs its order to be preserved consume much computer resources during communication, which can eventually result in lags. In Yorkie, actions that do not need its order to be preserved were managed as Presence, and those that needed its order to be preserved (= that is in need of management through CRDT algorithms) were managed as Data. In a whiteboard application, the real-time action (interactions that the user commits to the whiteboard while their mouse is down) do not really need to be communcated with its order preserved since their actions are not finished, and thus can be managed as Presence. However, the committed action, which is anfinished action of a user (mouse up after mouse down) should have its order preserved since it is a finished action. Those were managed as Data.\n\n![User interaction recorded as presenc](/assets/project/toonie/soc_step1.png)\n\n![User interaction finishes and presence is reset](/assets/project/toonie/soc_step2.png)\n\n![Presence modifications are transferred to data](/assets/project/toonie/soc_step3.png)\n\nSeparation of concerns was used for implementing the distinction between Presence and Data, and thus multiple layers (presence canvas and data canvas) was designed for the whiteboard application. User's real-time unfinished interactions (whiteboard modifications that occur while the user's mouse is pressed) were drawn in the Presence canvas. As soon as the user's interaction finishes (user finishes modification by mouseup), then the presence canvas data is reset and the modification data is sent to the Data Canvas. By doing so, I was able to design a software design approach for optimizing collaborative whiteboards.\n","categories":["opensource","collaboration","CRDT"],"thumbnail":"/assets/project/toonie/thumbnail.png","WIP":false,"data":{"title":"Toonie: Real-time Collaborative image review editor","excerpt":"Toonie is a real-time CRDT-based collaborative image review editor where users can review on images uploaded to the service together. I developed software design appraoches for optimizing collaborative whiteboards.","startDate":"2022-06-27","date":"2022-08-19","author":{"name":"Kim Dong Hun"},"keyword":"collaboration","categories":["opensource","collaboration","CRDT"],"thumbnail":"/assets/project/toonie/thumbnail.png","WIP":false}},{"title":"Persona Personality: Discover the personality masks of my friends!","date":"2021-06-01","slug":"personapersonality","excerpt":"Persona Personality is an Enneagram-based personality test for figuring out the personality of my acquaintances. The personality results are returned as a form of a maks. While figuring out the personality of others, one can have a deeper understanding of their own personality by reviewing what type of people they hang out with.","keyword":"enneagram","content":"\n[View Website →](https://www.personapersonality.com/)\n\nIn 2021, young people (mostly 20s) in Korea were crazy with MBTI (Myers-Briggs Type Indicator). MBTI divided human personalities into 16 types. It figures out one's personality by discovering one's preference in four categories: Extroversion(E) vs. Introversion(I), Sensing(S) vs. Intuition(N), Thinking(T) vs. Feeling(F), and Judging(J) vs. Perceiving(P). In each of the four categories, one can be either one of the two types. For example, one can be either Extroversion(E) or Introversion(I). This simple and easy-to-understand personality test enabled young Korean people to understand themselves. More and more MBTI test websites were created daily, and their contents were constantly consumed by many young people.\n\nHowever, I noticed a pattern of misuse of MBTI results among many young people. MBTI is known as one of the tests that has its basis in 'positive psychology' - it was intended to enable people to see positive parts of oneself. Unfortunately, young people started to treat MBTI as an ultimate truth and started to use it as a tool to define limitations on oneself. For instance, if a person found out that he/she is classified as I (Introversion) rather than E (Extroversion) in MBTI's first category, the person would think that he/she is not good at socializing with others and use that result to explain to others why he/she cannot do something that is about presenting oneself in front of others. MBTI results were starting to be used as an excuse or justifications to one's limitations.\n\nThe main reason to this phenomenon was because most MBTI web-based tests were focused on identifying oneself. It was intended to give a single definition on one's personality with a dichotomic approach. To remedy this phenomenon, a test that allows one to realize that one's personality cannot be defined into a single statement, which is in fact true, was necessary. Thus, I contrived a new personality test that rather figures out the personality of one's acquaintances rather than oneself. The recepient of the personality result could receive multiple interpretations of his/her own personality from others, and realize that one's personality is a malleable concept that cannot be defined into a single statement. Even the sender of the personality result can have a deeper understanding of his/her own personality by reviewing what type of people he/she hangs out with.\n\n![9 Mask Results in Personapersonality](/assets/project/personapersonality/masks.png)\n\nPersonapersonality is the application that I created for this purpose. Its pyshcology test basis is Enneagram, which is a personality trait theory that divides human personalities into 9 types. The reason Enneagram was chosen instead of MBTI was because 1) MBTI had a limited explanation on one's personality while Enneagram explained one's personality holistically, and 2) I wanted to prevent people from interpreting the results in the MBTI point of view. The questions asked in the application were based on the user's acquaintance, and questions such as \"What do you think your (acquaintance's name) would do when he has to lead a classroom conference?\". After answering custom curated 15 questions on the other person, one could get the enneagram result of the other person. The personality result was presented as a form of a mask, which is a visual representation of the personality type. This was intentional since one's personality can be understood differently based on how the person interacts with that person. Thus, the name 'Persona' personality was given to the application.\n\n![Question Screen](/assets/project/personapersonality/question.png)\n\n![Mask Result Screen](/assets/project/personapersonality/mask_result.png)\n\nWays to maintain a good relationship with other people were provided alongside the explanation of the other person's personality, and this was because the application itself was intended to be used as a tool to understand others better. Also, since the initial goal was to preven people from understanding one's personality as one single truth, I also provided a data visualization of the analyzed results, where one could check the percentage of each personality type that was given to the person. This was to show that one's personality is not a single truth, but rather a combination of multiple personalities.\n\n![Personality Mask Result Analysis](/assets/project/personapersonality/result_analysis.png)\n\nAs of today (2023.07.18), 204495 people have used the application to figure out the personality of their acquaintances.\n","categories":["personality-test"],"thumbnail":"/assets/project/personapersonality/thumbnail.png","WIP":false,"data":{"title":"Persona Personality: Discover the personality masks of my friends!","excerpt":"Persona Personality is an Enneagram-based personality test for figuring out the personality of my acquaintances. The personality results are returned as a form of a maks. While figuring out the personality of others, one can have a deeper understanding of their own personality by reviewing what type of people they hang out with.","startDate":"2020-09-01","date":"2021-06-01","author":{"name":"Kim Dong Hun"},"keyword":"enneagram","categories":["personality-test"],"thumbnail":"/assets/project/personapersonality/thumbnail.png","WIP":false}}],"projectCategories":["Data Visualization","Policy","hackathon","AI","editor","data-visualization","social-network","opensource","collaboration","CRDT","personality-test"]},"__N_SSG":true},"page":"/projects","query":{},"buildId":"35x8J-pMSzD2W4df1IM-v","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>