{"pageProps":{"allPosts":[{"title":"3D Parametric curves and surfaces for computer visualization","date":"2024-10-18","slug":"parametric_surface","author":{"name":"Kim Dong Hun"},"excerpt":"Understanding parametric curves and surfaces is essential for visualizing equations in 3D space. This post explores how parametricization enables the representation of complex intersections and surfaces, with practical coding examples for computer graphics.","keyword":"Parametric Curves","content":"\n## Representing a curve in $\\mathbb{R}^3$\n\nWe all know how to create a line or a curve in $\\mathbb{R}^2$ using the equation $y = f(x)$. We just need to have a single equation that have two variables in it. What happens if you want to represent a curve in $\\mathbb{R}^3$? We can use the concept of parametric curves to represent a curve in $\\mathbb{R}^3$.\n\nHowever, creating a curve in $\\mathbb{R}^3$ is not as simple as creating a curve in $\\mathbb{R}^2$. Yes, if the function is simple enough like $z = x^2 + y^2$, then we can easily represent the curve in $\\mathbb{R}^3$. But what if we were given two implicit equations instead and required to find the intersecting line, such as $x^2 + y^2 = 1$ and $y=xz$? Finding a curve to represent the intersection of these two equations is not as simple as finding the intersection of two lines in $\\mathbb{R}^2$.\n\nIn fact, if you think this problem in a practical point of view, this is in fact how many of our social science models are introduced. We do not have a single equation that represent the behavior of the system. Instead, we have multiple equations that can represent the behavior of the system. Thus, being able to solve the intersection of these equations is a very important skill to have.\n\n## Parametric Curves\n\nNow to solve the intersection of the given equations (at least two should be provided for $\\mathbb{R}^3$), we can use the concept of parametric curves. The idea is to represent the equations with a common variable. For example, let us say we want to parameterize $x^2 + y^2 = 1$ and $y=xz$. At first glance, it seems impossible to represent the intersection of these two equations. However, if we use a common variable $t$ to represent the equations and change the euclidean coordinates into a radial coordinate, we can easily represent the intersection of the two equations.\n\n$$\n\\begin{align*}\nx &= 5\\cos(t) \\\\\ny &= 5\\sin(t) \\\\\nz &= 25\\cos(t) \\sin(t)\n\\end{align*}\n$$ \n\nSince all $x$, $y$, and $z$ are represented in terms of $t$, the resulting visualization will be a curve. This is a parametric curve that represents the intersection of the two equations. Now thanks to representing the curve with a single variable, we can easily visualize the curve as a code!\n\n```c++\n// float t is a variable with a range of 0 to 2 * PI\n\nfloat x = 5 * cos(t);\nfloat y = 5 * sin(t);\nfloat z = 25 * cos(t) * sin(t);\n```\n\nI created a glsl shader that visualizes the curve. I first created a plane using Three.js and then created a shader that visualizes the curve. I set the t as `position.x` and then calculated the x, y, and z values using the equations above. The result is a beautiful curve that represents the intersection of the two equations.\n\n<ParametricSurface1 />\n\n## Parametric Surface\n\nThere are some cases we are not given sufficient information to express all $x$, $y$, and $z$ in terms of a single variable. In those cases, we would need two variables to parametricize the equations. Since the variable dimension is now 2, instead of having a curve, we will have a surface. The resulting surface is called a parametric surface. In most cases, the variables we will parameterize on are named $u$ and $v$. An example that requires us to use two variables to parametricize the equations would be the below equation:\n\n$$\n\\begin{align*}\nx^2 - y^2 + z^2 &= 0 \\\\\n\\end{align*}\n$$\n\nThere is no way we can use a single variable to parametricize the equation. We need to use two variables to parametricize the equation. There are two ways you can parametricize the equation. One way is to think of x and y being the two variables that we will use to parametricize the equation. Then we will have the following equations:\n\n$$\n\\begin{align*}\nx &= u \\\\\ny &= v \\\\\nz &= \\pm\\sqrt{u^2 - v^2}\n\\end{align*}\n$$\n\nHowever, the above equation is problematic in that we need to use the $\\pm$ sign to represent the equation. In terms of coding, this will mean that we need to create two curves to represent the equation. We can solve this by parametricizing the equation in a different way, which is to use radial coordinates.\n\n> In most cases, in the case of $\\mathbb{R}^3$, when there are two squared terms that are added together, we can use radial coordinates to parametricize the equation. In the case of the equation $x^2 - y^2 + z^2 = 0$, we can change x and z into radial coordinates since they are squared terms that are added together.\n\n> Remember that changing a cartisian coordinate into a radial coordinate is done by using the following equations: $x = r\\cos(u)$, $y = r$, and $z = r\\sin(u)$.\n\n$$\n\\begin{align*}\nx &= v\\cos(u) \\\\\ny &= v \\\\\nz &= v\\sin(u)\n\\end{align*}\n$$\n\nThe reason why we add an v in front of the $\\cos$ and $\\sin$ functions is because according to the equation $x^2 - y^2 + z^2 = 0$, the radius of the curve is $y$. Now with the parametricized equations expressed in terms of u and v, we can easily visualize the curve using the code below:\n\n```c++\n// float u and v are variables with a range of 0 to 2 * PI\nfloat x = v * cos(u);\nfloat y = v;\nfloat z = v * sin(u);\n```\n\n<ParametricSurface2 />\n\nSimilar to the above coding, I used the x and y in the plane geometry of Three.js as u and v and then calculated the x, y, and z values using the equations above.\n\nWe can creatively experiment on new forms such as a pasta with the correct equations. We can visualize a pasta using a parametric surface described with radial coordinates. \n\n$$\n\\begin{align*}\nr = 15 - 9 \\cos(x/3) \\\\\n\\end{align*}\n$$\n\nWith the above equation, since $x$ is in the equation already, it would be nice if we could think of $x$ as $u$. We can think of the $\\theta$ as $v$ since it is not given. Then we can parametricize the equation as follows:\n\n$$\n\\begin{align*}\nx &= u \\\\\ny &= (15 - 9 \\cos(u/3)) \\sin(v) \\\\\nz &= (15 - 9 \\cos(u/3)) \\cos(v) \\\\\n\\end{align*}\n$$\n\nThe resulting visualization is a pasta that is represented by the parametric surface.\n\n<ParametricSurface3 />\n\n\n\nParametric curves and surfaces are an interesting concept that is good to have in your toolbox especially for computer graphics and art. If it is difficult to imagine why we need to learn parametricization, just simply remember that it is through parametricization that we can code the surfaces in software!\n","categories":["Computer Graphics"],"thumbnail":"/assets/posts/parametric_surface/pasta_surface.jpg","WIP":false,"data":{"title":"3D Parametric curves and surfaces for computer visualization","excerpt":"Understanding parametric curves and surfaces is essential for visualizing equations in 3D space. This post explores how parametricization enables the representation of complex intersections and surfaces, with practical coding examples for computer graphics.","date":"2024-10-18","author":{"name":"Kim Dong Hun"},"keyword":"Parametric Curves","categories":["Computer Graphics"],"WIP":false,"thumbnail":"/assets/posts/parametric_surface/pasta_surface.jpg"}},{"title":"Linear Regression Model Selection (feat. Geomertry)","date":"2024-10-10","slug":"linear_regression_geometry","author":{"name":"Kim Dong Hun"},"excerpt":"Linear regression models can struggle with too many predictors, leading to overfitting. This post delves into Ridge and Lasso regression, explaining their role in model selection through intuitive geometric perspectives.","keyword":"Model Selection","content":"\n## Linear Regression Fails with Many Predictors\n\nLinear regression is powerful in that it allows us \"humans\" to understand what is happening in the model and how the model reasoned to make a prediction. This is because Linear regression essetially outputs a linear equation like below:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n\n$$\n\nwhere $y$ is the output variable, $x_1, x_2, \\cdots, x_n$ are the input variables, and $\\beta_0, \\beta_1, \\cdots, \\beta_n$ are the coefficients of the input variables.\n\nHowever, it gets complicated when we are trying to introduce many input variables, sometimes more variables than the number of observed samples. In such case, simple linear regression model that uses all the input variables (Ordinary Least Squares) can overfit the data and output poor results.\n\n## Model Selection with Ridge and Lasso\n\nTo remedy cases where we have many input variables, there are mainly three methods we can use to create another linear regression model.\n\n1. **Forward and Backward Selection**: This method selects the best subset of input variables that can explain the output variable. It is a brute-force method that tries all possible combinations of input variables and selects the best subset.\n\n2. **Shrinkage method**: This method shrinks the coefficients of the input variables to zero. This method is useful when we have many input variables and we want to select only the important variables. There are two types of shrinkage methods: Ridge and Lasso.\n\n3. **Dimension Reduction**: This method reduces the dimension of the input variables. This method is useful when we have many input variables and we want to reduce the number of variables. One example of this would be Primary Component Analysis (PCA).\n\nIn this post, we will focus mainly on shrinkage method and introduce them not as how it is normally explained but by using geometric images to explain what is happening in them.\n\n## Brief Introduction of Ridge and Lasso\n\nTo first understand Ridge and Lasso, one should have a good understanding of Ordinary Least Squares, which is the most basic version of linear regression model.\n\nThe ultimate goal of Ordinary Least Squares is to find the coefficients ($\\beta_0, \\beta_1, \\cdots, \\beta_n$) of the predictors that make the residual sum of squares (RSS) to 0. The Residual sum of squares is calculated as below:\n\n$$\nRSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\nwhere $y_i$ is the actual output value and $\\hat{y}_i$ is the predicted output value.\n\nSince OLS goal is just to minimize the difference between the actual and predicted output values, it can be said that OLS has the least bias.\n\n### OLS and MSE?\n\nOK. Now frankly speaking, many already know MSE (Mean Squared Error) since it is a term that appears both in statistics and machine learning. But what is OLS? It seems that they are connected but how?\n\nSo Ordinary Least Squares (OLS) can be best understand when contrasted with other methods that have similar names. Other than OLS, there is also a a WLS which is Weighted Least Squares. So, both have least squares in their names and essentially they compare between the actual and predicted values whose difference is squared for later minimization. However, WLS applys weights to the squared differences to specific samples. Essentially its goal is to minimize the weighted sum of squared differences:\n\n$$\nMinimize \\sum_{i=1}^{n} w_i(y_i - \\hat{y}_i)^2\n$$\n\nwhere $w_i$ is the weight applied to the $i$-th sample.\n\nOn the other hand, OLS does not apply any weights to the squared differences. It just tries to minimize the sum of squared differences:\n\n$$\nMinimize \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\nIn fact, the 'Ordinary' in OLS means that it does not apply any weights to the squared differences. It just tries to minimize the sum of squared differences. (For your information $\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$ is also called the Residual Sum of Squares (RSS).)\n\nSo how does Mean Squared Error (MSE) come into play? Well, MSE is essentially the average of the squared differences between the actual and predicted values. So it can be said that MSE is the average of the RSS. Thus, it can be said that OLS is the method that tries to minimize the MSE.\n\n$$\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\n### Calculation of Bias and Variance\n\nAllright, so we have been told that OLS has the least bias among linear regression models. Let us talk about the bias and variance of it.\n\nNow when talking about models, bias and variance are two keywords that always show up. Essentially in predictive models, there is always a bias and variance tradeoff. The most famous image that explain both bias and variance is the image below:\n\n![Bias Variance](/assets/posts/linear_regression_geometry/bias_variance.png)\n\nIn the image, the bullseye is the target. The lower the variance, the denser the shots are to each other (regardless of its proximity to the bullseye). The higher the bias, the more offset the shots are from the bullseye. In linear regression, it is a common practice to find a model that has the least bias and variance, which is in fact difficult due to the tradeoff that exists between them.\n\nNow how to calculate the variance is relatively easy. It is the standard deviation of the predicted values themselves. In calculating the variance, there is no need to rely on the actual values. The variance can be calculated as below:\n\n$$\nVar(\\hat{y}) = E[(\\hat{y} - E[\\hat{y}])^2]\n$$\n\n$$\nVar = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - E[\\hat{y}])^2\n$$\n\nwhere $E[\\hat{y}]$ is the expected value of the predicted values.\n\nAs you can see, there is no need to rely on the real observed $y$ values. The variance can be calculated just by using the predicted values.\n\nHowever, bias is a bit more complicated. Bias is the difference between the expected value of the predicted values and the actual values. Thus, it requires us to have our observed $y$ values. The bias can be calculated as below:\n\n$$\nBias(\\hat{y}) = E[\\hat{y}] - y\n$$\n\n$$\nBias = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)\n$$\n\nwhere $y$ is the actual output values.\n\n### OLS has the least bias?\n\nSo, why is OLS said to have the least bias in a mathematical perspective? Let us remember that OLS attempts to minimize the below function\n\n$$\nRSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\nFor exploaratory purposes, we can safely say that minimizing $RSS$ is the same as minimizing $MSE$ which is\n\n$$\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\nIn fact, if we calculate the MSE in $E$ term it is,\n\n$$\nMSE = E[(y - \\hat{y})^2]\n$$\n\nwhich eventually leads to the interpretation of MSE as a function of bias and variance. You may look at how the below function is derived through this [link](https://www.geeksforgeeks.org/bias-vs-variance-in-machine-learning/)\n\n$$\nMSE = Bias^2 + Var + \\sigma^2\n$$\n\nwhere $\\sigma^2$ is the irreducible error.\n\nIf assumptions on the linear model are held, then the bias of the OLS model is 0 according to the Gauss-Markov theorem. The assumptions are 1) the model is linear in the parameters, 2) the errors of residuals are independent and identically distributed, 3) the errors are normally distributed (mean should be 0), and 4) the errors have a constant variance. If these assumptions are held, then the OLS model has the least bias. The specifics of Gauss-Markov theorem can be found in this [link](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem). But just to give you a brief overview, In the final equation of the theorem, the OLS model shows that the mean of the estimated coefficients is equal to the true coefficients.\n\n$$\nE[\\hat{\\beta}] = \\beta\n$$\n\nIf the above is held, the the model can be said to have a bias of 0.\n\n## Ridge Regression\n\nWe have come a long way to really discuss about Ridge Regression and Lasso Regression. So Ridge Regression and Lasso manipulates the bias and variance of the model by adding a penalty term to the OLS model. The penalty term is added to the original RSS and the goal of the Regression model changes to minimizing the below function:\n\n$$\nMinimize \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n$$\n\nSo as you can see here, a new term $\\lambda \\sum_{j=1}^{p} \\beta_j^2$ is added to the RSS function. This term is called the penalty term. The penalty term is the sum of the squares of the coefficients of the input variables. The $\\lambda$ is the hyperparameter that controls the strength of the penalty term. If $\\lambda$ is 0, then the penalty term is 0 and the model is the same as the OLS model. If $\\lambda$ is very large, then the penalty term is very large and the coefficients of the input variables are shrunk to 0.\n\nIn easy words, Ridge Regression introduces a constraint `budget` that the RSS should be operated within. Remind yourself that this is a constraint! The whole point of Ridge Regression is to find the coefficients ($\\beta_0, \\beta_1, \\cdots, \\beta_n$) that minimize the RSS while keeping the sum of the squares of the coefficients within the budget. This is why Ridge Regression is also called L2 regularization.\n\nThis is in fact an optimization problem! We are given a budget and we need to find the point where the RSS is minimized while keeping the sum of the squares of the coefficients within the budget.\n\nLet us understand this easily using geometry. For simplifying the problem let us imagine a two predictor linear regression model. This model will be expressed as below:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n$$\n\nOur goal in Ridge Regression is to find the coefficients ($\\beta_0, \\beta_1, \\beta_2$) that minimize the RSS while keeping the sum of the squares of the coefficients within the budget. But before we go into plotting lines and curves to understand how Ridge Regression works, let us first understand OLS in a geometric perspective.\n\n### OLS in a Geometric Perspective\n\nSince we have a two predictor linear regression model, we can plot the combination of all the coefficients into a 2d plane. OLS model will give us only one single combination in the 2d plane, since there is only one combination that can minimize the RSS in the linear regression model. This is the combination that is closest to the actual output values.\n\n![OLS coefficient combination for two predictors](/assets/posts/linear_regression_geometry/ridge_lasso1.png)\n\n### Ridge Regression in a Geometric Perspective\n\nOLS was relatively easy to understand. Now how about Ridge Regression? First remind that Ridge Regression is minimizing the RSS within a fixed budget. Thus it is an optimization probelm. Our budget can be depicted as an area in the 2d plane. Plus, since our ridge regression is a square of the coefficients, the budget area will be a circle.\n\n![Ridge Regression budget shown as an area](/assets/posts/linear_regression_geometry/ridge_lasso2.png)\n\nNow the dynamic variable is only the RSS. We need to sacrifice RSS to find the right coefficient combination that is within the budget (=the point should fall in the area of the circle). As we sacrifice more and more of the RSS, the coefficients will change. What happens if we sacrifice RSS? The RSS will be a value higher than 0 and thus, many coefficients can satisfy to meet such RSS. For instance for a RSS=1, the coefficient combinations will be shown as a line in the 2d plane. All combination points on the line will have a RSS of 1.\n\n![Ridge Regression budget shown as an area](/assets/posts/linear_regression_geometry/ridge_lasso3.png)\n\nHowever, remember that though the coefficients can get larger than the original RSS=0 coefficients, we only care about the smaller coefficients since they are the ones that are highly likely to be met in the budget area. This is why Ridge Regression is also called a `shrinkage method`. The coefficients are shrunk to 0 to find the right combination that is within the budget. Since none of the points in RSS=1 line is within the budget, we need to continue sacrificing the RSS to find the right combination.\n\n![Ridge Regression budget shown as an area](/assets/posts/linear_regression_geometry/ridge_lasso4.png)\n\nAs we continue to sacrifice the RSS, the line of possible combinations of coefficients will be larger and larger. Eventually, we will find the right combination that is within the budget. This is the combination that minimizes the RSS while keeping the sum of the squares of the coefficients within the budget. Since both the area and our line are ellipses, the combination that is the best combination will be the point where both ellipses intersect to each other.\n\nThough we can sacrifice the RSS more and find more combinations that can fit within the boundaries, we have to remember that this is an optimization problem. Thus, the combination that is the best is the one that minimizes the RSS while keeping the sum of the squares of the coefficients within the budget. This is the combination that is the closest to the actual output values. This is in fact the point where the Ridge Regression model stops its exploration and outputs the coefficients.\n\n![Ridge Regression budget shown as an area](/assets/posts/linear_regression_geometry/ridge_lasso5.png)\n\n### Lasso Regression in a Geometric Perspective\n\nNow how is Lasso Regression expressed in the coordinates. First, we should remind ourselves how lasso regression is different from ridge regression. Lasso regression is also a shrinkage method, but it uses the sum of the absolute values of the coefficients as the penalty term. This is why Lasso Regression is also called L1 regularization.\n\nThe penalty term of Lasso Regression is expressed as below:\n\n$$\nMinimize \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n$$\n\nNow the penalty term is different from the Ridge Regression. Thus the budget area will not be shown as an ellipse but a rectangular. Everything else is the same. We need to sacrifice RSS untill there is a point that is within the budget area.\n\n![Ridge Regression budget shown as an area](/assets/posts/linear_regression_geometry/ridge_lasso6.png)\n\nIf you see the intersecting point, there is an interesting thing that happens. The Lasso Regression model can output a combination that has some of the coefficients to be 0. This is due to the shape of the budget area. In Ridge regression, the budget area was an ellipse, and thus it is hihgly unlikely for the intersecting points to end up on the axes. However, in Lasso Regression, the budget area is a rectangle, and thus it is highly likely for the intersecting points to end up on the axes. Due to this, Lasso Regression can output a combination that has some of the coefficients to be 0.\n\n## Conclusion\n\nThis is the end of the explanation of Ridge and Lasso Regression in a geometric perspective. I hope that this explanation was helpful in understanding how Ridge and Lasso Regression works. I would like to thank Professor [Soroush Saghafian](https://scholar.harvard.edu/saghafian) for helping me understand this topic!\n\n## Reference\n\n1. An Introduction to Stastistical Learning\n","categories":["Machine Learning"],"thumbnail":"/assets/posts/linear_regression_geometry/ridge_lasso4.png","WIP":false,"data":{"title":"Linear Regression Model Selection (feat. Geomertry)","excerpt":"Linear regression models can struggle with too many predictors, leading to overfitting. This post delves into Ridge and Lasso regression, explaining their role in model selection through intuitive geometric perspectives.","date":"2024-10-10","author":{"name":"Kim Dong Hun"},"keyword":"Model Selection","categories":["Machine Learning"],"WIP":false,"thumbnail":"/assets/posts/linear_regression_geometry/ridge_lasso4.png"}},{"title":"VAE the basics","date":"2024-03-02","slug":"vae_overview","author":{"name":"Kim Dong Hun"},"excerpt":"Variational Autoencoders (VAEs) extend traditional autoencoders by incorporating probabilistic elements, enabling better reconstruction and generative capabilities. This post provides an in-depth look at VAEs, from their architecture and reparameterization trick to the mathematics behind their loss functions.","keyword":"VAE","content":"\n## What is VAE for?\n\nVariational Autoencoder (VAE) is known to be a more advanced Autoencoder (AE). The main difference between AE and VAE is that VAE has an additional statistical feature added to it in the middle. Both AE and VAE network inputs multi-dimensional data and outputs a multi-dimensional output that has a high resemblance to the inputted data. Thus, it means that AE and VAE are networks that try their best to output the same data that was initially inputted. How they train the parameters for making such output is through an encoder and decoder network, where the encoder tries to compress the multi-dimensional input into smaller data and make the decoder recreate(=reconstruct) the image with that compressed data. In the AI realm, this compressed data is called a vector in a latent space.\n\n![Encoder_Decoder](/assets/posts/vae_overview/encoder_decoder.png)\n\nThough AE and VAE have similar structures, their purposes differ. While AE's main focus is on the encoder, the VAE's main focus is on the decoder. This means that AE's purpose is to compress the multi-dimensional data into a compressed vector while VAE's purpose is to generate images. For instance, AE can be mainly used to create an image-based search system where every uploaded image goes through the encoder network to output a vector composed of numbers, which will be used to find similar images compared to a newly inputted image. VAE can be used for generative purposes such as generating a new image.\n\nIn this post, the main focus is to get a glimpse into how VAE works. As mentioned above, the feature that makes VAE more advanced compared to AE is that it has a statistical feature added in the middle of the network.\n\n![VAE](/assets/posts/vae_overview/vae.png)\n\n## The additional statistical feature in VAE\n\nSo I have previously said that there is an additional statistical feature in VAE. Why then is a probabilistic layer added to the latent space? VAE's purpose is to `reconstruct well`. Under the hood, VAE understands the inputted training samples as data with irregular and random noises. Thus a probabilistic layer on the latent space can filter out the noise that has not much to do with the [`inherent signal`](https://medium.com/tensorflow/variational-autoencoders-with-tensorflow-probability-layers-d06c658931b7) of the data.\n\nIn the VAE network, this probabilistic layer is implemented by extracting a `μ vector` and a `σ vector`, each representing the mean and standard deviation of the input data. In many practices, the VAE network is assumed to have a normal distribution, μ is 0 and σ is 1. Thus in the training process, while extracting two vectors, one for μ and one for σ, we must take measures to make the μ and σ follow the normal distribution.\n\nI first was curious whether it is possible to extract the μ and σ of a multi-dimensional input. Was there a mathematical function to get the μ and σ of any encoded data? However, after looking through some codes, it became evident that there does not exist a specific mathematical approach to calculating the μ and σ. In most networks, μ and σ are simply vectors that go through a linear layer in the encoder network. It is the loss function's job to make the linear layers to make the two vectors the μ and σ of a normal distribution. The element that makes the two vectors into the μ and σ of a normal distribution is the KL divergence loss element, which is something that is discussed in the later paragraphs.\n\n```python\nclass Encoder(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super(Encoder, self).__init__()\n\n        self.FC_input = nn.Linear(input_dim, hidden_dim)\n        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n\n        self.LeakyReLU = nn.LeakyReLU(0.2)\n\n        self.training = True\n\n    def forward(self, x):\n        h_       = self.LeakyReLU(self.FC_input(x))\n        h_       = self.LeakyReLU(self.FC_input2(h_))\n        mean     = self.FC_mean(h_)\n        log_var  = self.FC_var(h_)\n        return mean, log_var\n```\n\nAs you can see in the above code, there are not any mathematical measures added to make mean and variance vectors.\n\n## Sampling a latent space vector\n\nNow that we got a hang of the meaning of a probabilistic layer inside a VAE, it is now time to understand how the latent space vector is sampled. In short, how is a latent space vector, z, created through the extracted mean and variance vectors? The best method is to randomly choose a vector from the normal distribution represented by the mean and variance vectors. However, when a latent space vector, z, is created through such a process, there is no way the VAE model can update its parameters through backpropagation. The selection of a latent space vector through the statistical method is `not differentiable`.\n\nResearchers instead introduced a new approach to create a latent space vector out of the mean and variance vector. What they did was simply use arithmetic operations to create a latent space vector. $z=\\mu+\\sigma*\\epsilon$. $\\epsilon$ here is a randomly chosen vector in a normal distribution. This method is called the `Reparameterization Trick`. The math behind this trick is very complex and I will not go into detail about the math.\n\n## Math behind VAE\n\nFor a neural network to train its parameters, a clear purpose of the model should be set. As mentioned above, the main goal of the VAE is to `well reconstruct` the original input through the decoder. Thus, if we say the parameters of the decoder network is $\\theta$, our goal is to make the probability of x, original, maximum, $p_\\theta(x)$. We use our latent space vector, z to accomplish this, so the function should take z into consideration. If we change the function to incorporate z, it is $\\int p_\\theta(z)p_\\theta(x|z) \\, dz$. The reason $p_\\theta(x|z)$ appeared is because the combined probability of x and z can be represented as x|z. $\\frac{p_\\theta(x,z)}{p_\\theta(z)} = p_\\theta(x|z)$, which also means $p_\\theta(x,z)=p_\\theta(z)\\times p_\\theta(x|z)$. We know that $\\int p_\\theta(x,z) \\, dz = p_\\theta(x)$. In fact, $p_\\theta(x|z)$ is the representation of the decoder network of VAE.\n\nHowever, the problem is that the integral for $p_\\theta(x|z)$ cannot be calculated(=intractable). In the context of integral, having that $p_\\theta(x|z)$ means that we have to calculate the probability of x for all latent space vector z. Researchers thought that if $p_\\theta(x|z)$ cannot be directly, why not find the posterior distribution, distribution of latent variables given the observed data, $p_\\theta(z|x)$? In addition to escaping the burden of finding the $p\\theta(x|z)$, since the VAE's job is to generate outputs that are highly similar to the inputted data, a well-structured latent space can ensure continuity and completeness. `This is why a VAE necessitates the training of an encoder despite its main focus on the decoder`.\n\nUnfortunately, finding $p_\\theta(z|x)$ is also not an easy task since finding $p_\\theta(z|x)$ also necessitates a prior knowledge of $p_\\theta(x)$. $p_\\theta(z|x)=\\frac{p_\\theta(x|z)\\times p_\\theta(z)}{p_\\theta(x)}$. Thus we need to accept the fact that we cannot find the $p_\\theta(z|x)$ directly, but instead train an encoder network that is close to $p_\\theta(z|x)$. The encoder network will be represented as $q_\\phi(z|x)$.\n\nSo back to the original purpose of VAE. Our goal is to maximize the data likelihood, $p_\\theta(x)$. Let us add a log to the value to make our calculations simple. $\\log{p_\\theta(x^{(i)})}$. i is a value from 1 to N, and N represents the number of input data. To make the calculation simple we make the value as Expectation, $E_{z \\sim q_\\phi(z|x^{(i)})}[\\log p_\\theta(x^{(i)})]$. Here, z follows the distribution of our encoder($q_\\phi(z|x^{(i)})$). According to Bayes' rule, we can change the $p_\\theta(x^{(i)})$ into $\\log \\frac{p_\\theta(x^{(i)}|z)p_\\theta(z)}{p_\\theta(z|x^{(i)})}$. The following math is shown in the image below.\n\nBayes rule\n\n$$\np(z|x)=\\frac{p(x|z)\\times p(z)}{p(x)}\n$$\n\n![VAE](/assets/posts/vae_overview/vae_math.png)\n\nAs shown in the image, in the end, we end up with three terms we need to consider when maximizing $\\log{p_\\theta(x^{(i)})}$. Unfortunately, since $p_\\theta(z|x^{(i)})$ is intractable, the last KL term cannot be calculated. However, due to KL divergence outputting a result that is always >=0, we can kindly disregard this term. The initial two terms combined are called the `Variational lower bound(=ELBO)` in a VAE, and our goal is to maximize the ELBO. The first term in ELBO is related to the decoder network and the second term in the ELBO is related to the encoder.\n\n### What we need to find\n\n$$\n\\{\\theta^*, \\phi^*\\} = \\operatorname{arg\\,max}_{\\theta, \\phi} \\sum_{i} \\left( \\mathbb{E}_{q_\\phi(z|x_i)} \\left[\\log p_\\theta(x_i|z)\\right] - \\mathrm{KL}\\left(q_\\phi(z|x_i) \\parallel p(z)\\right) \\right)\n$$\n\nSince loss functions are often expressed as minimizing the function. we can change the above expression into the loss function below\n\n$$\n\\{\\theta^*, \\phi^*\\} = \\operatorname{arg\\,min}_{\\theta, \\phi} \\sum_{i} \\left( -\\mathbb{E}_{q_\\phi(z|x_i)} \\left[\\log p_\\theta(x_i|z)\\right] + \\mathrm{KL}\\left(q_\\phi(z|x_i) \\parallel p(z)\\right) \\right)\n$$\n\nThe first term has to do with reconstruction error, where it outputs the error to how much the $x_i$ was wrongly reconstructed, and the second term has to do with the regularization error, where it outputs the encoder's deviation from the normal distribution $p(z)$ (remember that we initially supposed the distribution of latent space vector z to be a normal distribution)\n\n## Analyzing the loss function terms\n\nMinimizing the loss in regularization is done by the KLD operation for two distributions. Since the $p(z)$ was set to a normal distribution, the function is relatively simple (I will not discuss the specifics of how KLD is calculated)\n\n![VAE regularization calculation](/assets/posts/vae_overview/vae_regularization.png)\n\nMinimizing the reconstruction error is a little bit more complex due to the necessity to take probabilities into consideration. For this purpose, the researchers used a Monte Carlo technique to simplify the calculation of integral. Also, the L was set to 1 to make the calculation even simpler, resulting in a reconstruction loss calculation to $\\log(p_\\theta(x_i|z^{(i)}))$\n\n![VAE reconstruction calculation 1](/assets/posts/vae_overview/vae_reconstruction1.png)\n\nNow to further calculate $\\log(p_\\theta(x_i|z^{(i)}))$, the characteristic of the p should be determined. The probability can either follow a multivariate Bernoulli or a Gaussian distribution. In the case of bernoulli, $\\log(p_\\theta(x_i|z^{(i)}))$, the probabilities of each item in the $x_i$ should be multiplied. In logarithmic operation, multiplication can be considered as additions. Thus the below cross entropy can be calculated. By the way, $p_{i,j}$ is the decoder network output.\n\n![VAE Bernoulli](/assets/posts/vae_overview/vae_bernoulli.png)\n\n### References\n\n1. https://www.youtube.com/watch?v=GbCAwVVKaHY&list=LL&index=1\n2. https://medium.com/tensorflow/variational-autoencoders-with-tensorflow-probability-layers-d06c658931b7\n3. https://towardsdatascience.com/on-distribution-of-zs-in-vae-bb9a05b530c6\n4. https://medium.com/analytics-vidhya/variational-autoencoders-explained-bce87e31e43e\n5. https://sassafras13.github.io/ReparamTrick/\n6. http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf\n7. https://jaan.io/what-is-variational-autoencoder-vae-tutorial/\n","categories":["Machine Learning"],"thumbnail":"/assets/posts/vae_overview/vae.png","WIP":false,"data":{"title":"VAE the basics","excerpt":"Variational Autoencoders (VAEs) extend traditional autoencoders by incorporating probabilistic elements, enabling better reconstruction and generative capabilities. This post provides an in-depth look at VAEs, from their architecture and reparameterization trick to the mathematics behind their loss functions.","date":"2024-03-02","author":{"name":"Kim Dong Hun"},"keyword":"VAE","categories":["Machine Learning"],"WIP":false,"thumbnail":"/assets/posts/vae_overview/vae.png"}},{"title":"Sky Palette Project: 2. Sky Images are Different","date":"2023-08-20","slug":"sky_palette_2","author":{"name":"Kim Dong Hun"},"excerpt":"In the second phase of the Sky Palette Project, I explored advanced techniques like PCA decomposition and SNIC segmentation to better capture the intricate details of sky colors, uncovering challenges and new possibilities in extracting harmonious palettes.","keyword":"vision","content":"\nThis post is a continuation of the [Sky Palette Project 1](./sky_palette_1).\n\n## PCA Decomposition of the sky image\n\n![2D PCA Decomposition](/assets/posts/sky_palette/pca2.png)\n\nContinuing where I took off last night, I started to go deeper into the analysis methods for the sky image. Last time, I tried with K-means clustering. However, since K-means clustering was simple average of the pixel rgb colors, I thought it was not the best way to capture the expressiveness of the sky image.\n\nI first started with analyzing the PCA components of the image. They returned some interesting results, where I could see a beautiful gradient of the sky image. I wanted to extract those very colors that could make the gradient.\n\n![1D PCA Decomposition](/assets/posts/sky_palette/pca1.png)\n\nI tried to express the colors in one dimension too. They were not as beautiful as the 2D PCA, but I could see some gradient in the colors. I thought that if I could extract the colors in the gradient, I could make a beautiful palette.\n\n![PCA extraction for 5 colors](/assets/posts/sky_palette/pca_extracted_5.jpg)\n\nI did make a 5 color palette from the pca components. The colors seemed to represent the important colors in the sky image, however, there was a slight lack in perfection. For this particular sky image, I especially liked the red and purple gradient part, but this palette did not capture that part. However, according to the PCA analysis of the colors, the purple color and the blue color were considered to be very similar. Thus, it was difficult to capture both the dark blue and dark purple colors in the palette.\n\n## SNIC segmentation of the sky image\n\nAfter much contemplation, I decided to try with another method. While I was staring at some sky images, I discovered that sky images had a positional difference in the colors. Since the sky image is basically a gradient, the colors in the sky image are different depending on the position of the image. I thought that if I could capture the positional difference, I could make a better palette.\n\n![SNIC segmentation of the sky image](/assets/posts/sky_palette/segmented_image.jpg)\n\nI started segmenting the sky image with a [SLIC algorithm](https://darshita1405.medium.com/superpixels-and-slic-6b2d8a6e4f08). The SLIC algorithm is a superpixel segmentation algorithm that segments the image into regions of similar colors. I thought that if I could segment the sky image into regions of similar colors, I could capture the positional difference in the sky image. I used the SLIC algorithm from the scikit-image library.\n\nAfter segmenting the image, I extracted the colors from each segment. However, during the process, I came across with a question. \"What if the sky image is not vertically aligned?\" I thought that if the sky image was not vertically aligned, the colors would be different depending on the position of the image. Thus, I started to find ways to align the sky image vertically.\n\n## Image Rotation with LAB Deconstruction\n\n![LAB Deconstruction](/assets/posts/sky_palette/lab_deconstruction.png)\n\nAfter much research, I found that LAB deconstruction is best for differentiating colors. So I first changed the rgb image into a lab image. After the deconstruction, I found that there were clear distinctions in color areas. Using a convolutional neural network, I calculated the gradients of each pixels. Then, I calculated the mean of the gradients to find the best rotation angle.\n\nThe rotation seemed to work. However, I soon found out that this was not applicable to all sky images. The biggest problem occurred when there were clouds. Since LAB takes lightness into its consideration, the clouds had a clear distinction with the atmosphere background color. Thus the rotation angle did not seem to work when there were many clouds. Thus I decided to forgo the attempt to align the sky image.\n\nI started to go deeper into color segmentation using SNIC. Assuming that the sky image has a vertical color gradient, I grouped the segments according to the vertical position of the segment. Then, I extracted the colors from each group.\n\n## Result Comparison\n\n![Kmeans & SNIC compare](/assets/posts/sky_palette/kmeans_snic_compare.png)\n\nThis is the result comparison of Kmeans (the algorithm I used in the last try) and SNIC. Now that the positional difference was considered, the colors seem to be more visually pleasing. However, if you see closely, you can see that Kmeans actually captured the representative colors better. While, the SNIC-produced colors seem to be more in harmony, there are some colors that SNIC could not catch while Kmeans could catch. Plus, SNIC works only when the sky image is vertically aligned.\n\nAfter seeing the results, I started to think that maybe I should not segment the sky image. Though positional information would be great, the positional information cannot be applied to all sky images. Moreover, positional information may lead to ignoring the impressive color parts in the sky image. For instance, if there is more blue in the sky image, most of the colors in the palette would be blue. I needed to think of another way to capture the colors in the sky image.\n\n## Solutions?\n\nAfter much thought, I started to see the problem from a different perspective. I was consumed with the idea of preserving the spatial information of the image, but it seemed useless. I looked at the 2D PCA decomposition again. If only I could capture the colors in the 2D PCA decomposition, I could make a beautiful palette.\n\n![Radial Selection of PCA](/assets/posts/sky_palette/radial_selection_pca.png)\n\nThen, I suddenly thought of a new way to group the images in the PCA. **Why don't I group the colors in the PCA radially?** Starting from the middle of the PCA, I could group the colors in the PCA radially. This would be great for capturing anomalies since the anomalies would be in the outer part of the PCA. I will try this method next time!\n\nYou can see my code for this project [here](https://github.com/hunkim98/sky-palette).\n","categories":["Computer Graphics","Machine Learning"],"thumbnail":"/assets/posts/sky_palette/segmented_image.jpg","WIP":false,"data":{"title":"Sky Palette Project: 2. Sky Images are Different","excerpt":"In the second phase of the Sky Palette Project, I explored advanced techniques like PCA decomposition and SNIC segmentation to better capture the intricate details of sky colors, uncovering challenges and new possibilities in extracting harmonious palettes.","date":"2023-08-20","author":{"name":"Kim Dong Hun"},"keyword":"vision","categories":["Computer Graphics","Machine Learning"],"WIP":false,"thumbnail":"/assets/posts/sky_palette/segmented_image.jpg"}},{"title":"Sky Palette Project: 1. The Beginning of the Journey","date":"2023-08-19","slug":"sky_palette_1","author":{"name":"Kim Dong Hun"},"excerpt":"The Sky Palette Project explores the extraction of harmonious color palettes directly from the sky, inspired by its natural gradients. This post details the journey from capturing sky images to applying K-Means clustering for initial color extraction.","keyword":"vision","content":"\n## The sky is the best color palette\n\n![Design Homework 2020](/assets/posts/sky_palette/design_hw.jpeg)\n\nIn a design basics class that I took in 2020, the professor gave students a homework to create a color palette out of their photos. The professor recommended us to take multiple photos that we preferred and extract the colors from them. Students were free to take new pictures if they wanted to. I did not take the class seriously, and I just used photos from my college life and extracted colors from them. After everyone submitted their homeworks, the professor told us one thing that I still remember to this day. She said, **When you have trouble finding the right colors to use in your design, look up to the sky and take a picture of it. The sky is the best color palette one can use because the color harmony is already there.** The sky is where light and darkness meet. The clouds add variance to the colors. The master of colors is right above your head, each day with a different palette.\n\nEver since I heard those words, I was always fascinated with the beauty of the sky. It is nature color untainted by human hands. Unfortunately, I never had a chance to use the colors of the sky. Design homeworks could be finished just by surfing through many reference images shared in the internet. I would glance at the sky when I was tired of my work. It was full of awe, but I was not ready to digest it.\n\nNow, transformed into a researcher interested in both Visual Communication Design and Computer Science, I began to experiment on the intersections of the two fields. While I was reading a dissertation on color palette recommendation and its application in design, the words of my design professor back in 2020 came back to me. In the dissertation, the color dataset for training the recommendation algorithm were extracted from visuals examined by design experts. I thought to myself, **\"Why do we need to rely on professionals for color recommendation? Can't the nature itself present us with the best color palette?\"** This is how I embarked on the journey of extracting colors from the sky.\n\n## Gathering data from the sky with a sky capturer\n\nTo extract colors from the sky, I first needed to capture the sky. I first approached this problem in a technological perspective. I created a web app that periodically captures images every interval.\n\n![Sky Capturer Web App](/assets/posts/sky_palette/sky_capturer.PNG)\n\nIn the web app, I prepared features where a user could set an interval time for capturing the sky image. I first thought of making the capture interval time static, but I realized that the sky would change its color rapidly when the sun is setting or rising. Thus, I connected the web app to an api that tells the sun set time and sun rise time according to the user's location. I made the web app change its capture interval time to 5 minutes when the current time is near the sun set or sun rise time. This was to capture the sky effectively when the sky is changing its color rapidly. The captured images were stored in a cloud storage.\n\n![Capturing Image at home](/assets/posts/sky_palette/capturing_image.jpg)\n\nI first tested the feasability of the web app in my home. I opened my web app in an iPad and started recording the sky. I set my iPad like the above image for one day. Thankfully, the iPad withstanded the heat of the sun. However, the captured image quality was poor. I could not fully capture the sky because there were buildings that were obstructing the view of the sky. Moreover, due to the window screen, the image was blurry. I needed to find a better place to capture the sky.\n\n![Captured Image Sample (Home)](/assets/posts/sky_palette/captured_at_home.jpeg)\n\n## Gathering sky image data from the internet\n\nI thought of capturing the image of the sky in another place. However, after much thought, I realized that I could search some images online. Though capturing images manually would be great, the first priority was to see if my sky palette project will work. Thus, I started surfing the web to find the adequate images for the sky palette project.\n\nI first searched for sky image datasets in google. Some datasets that were used for analyzing cloud patterns were available. Most datasets required a request to the owner of the dataset. Thus, I went on to request the dataset in multiple websites. I got some images from NUS (National University of Singapore). Vision & interaction group in NUS had gathered sky cloud images of Singapore for their [research](https://vintage.winklerbros.net/swimcat.html). I got the data from NUS, however, I soon found out that I could not use their image dataset for extracting colors. Most images were basically images of the clouds. Moreover, there seemed to have not much variance in the color of the sky. I needed to find another dataset.\n\n![SWIMCAT sample images](/assets/posts/sky_palette/swimcat.png)\n\nI kept searching google for image datasets. I founds some noteworthy datasets that were simply collected as a hobby. However, many of them had objects or artifacts other than the sky, which would make it difficult to extract the colors of the sky. I needed to find a dataset that was specifically made for the sky. I could go on and perform color segmentation to distinguish the sky from other objects, but that could be done after I had made a working prototype of the sky palette project. I needed to find a find dataset that I could use right away.\n\nAfter much surfing, I found an adequate dataset from a artist named [Eric Cahan](https://ericcahan.com/bio/). He was a photographer who had special interest in taking photos of beautiful skies. In his [Sky Series Selected Work](https://ericcahan.com/portfolio/sky-series/), there were some fabulous images that I could use to extract the color of the sky. Thus, I downloaded his images and started to extract the colors of the sky.\n\n## Color Extracting Algorithm Approach: K-Means Clustering\n\nI first approached the problem of extracting colors from the sky with a simple algorithm. I used K-Means clustering to cluster the colors of the sky. Many blogs explained that K-Means clustering is a simple algorithm that can be used to extract colors from an image. Thus, I first tried to use K-Means clustering to extract colors from the sky. I first tried to extract 5 colors from the sky.\n\n![Simple Kmeans Palette](/assets/posts/sky_palette/sky_palette_kmeans_initial.png)\n\nThe result was not too bad. But I felt it was not enough. I first tried extracting 5 colors. However, the 5 colors did not seem to harmonize well. What made it worse was that the 5 colors did not seem to represent the whole sky image. The sky image was beautiful and had many colors. Especially, the mix of red and blue was beautiful. However, due to Kmeans clustering method of taking the average of the colors, the represented colors were very dull.\n\nI thought that increasing the clusters would solve the problem. Though it did seem to improve, the colors were still dull. To me, when the cluster size was set to 15, the extracted colors seemed to represent the sky image well. I felt the need to tweak the Kmeans clustering algorithm to better extract the representative colors of the sky image. Also, in the hindsight, I felt that the ordering of the palette colors could have affected me to view the 5 clusters representation as a mispresentation of the sky image. Due to the random ordering of the colors, the colors seemed to not harmonized. If you see the sky image, you can see there is a gradual change in the color of the sky. However, once the images are seperated and presented discretly and color blocks, they look a bit awkward. I began to think that there is a need to order the colors as the sky image is ordered to lessen the awkwardness of the color blocks.\n\nI end this post here since my initial goals, gathering sky image data and going through basic kmeans algorithm approach, were sufficed. I will continue writing about this project in the next post. Anyone interested in the project can check out the [github repository](https://github.com/hunkim98/sky-palette)\n\nYou can read the next story in [Sky Palette Project (2)](./sky_palette_2)\n","categories":["Computer Graphics","Machine Learning"],"thumbnail":"/assets/posts/sky_palette/sky_palette_kmeans_initial.png","WIP":false,"data":{"title":"Sky Palette Project: 1. The Beginning of the Journey","excerpt":"The Sky Palette Project explores the extraction of harmonious color palettes directly from the sky, inspired by its natural gradients. This post details the journey from capturing sky images to applying K-Means clustering for initial color extraction.","date":"2023-08-19","author":{"name":"Kim Dong Hun"},"keyword":"vision","categories":["Computer Graphics","Machine Learning"],"WIP":false,"thumbnail":"/assets/posts/sky_palette/sky_palette_kmeans_initial.png"}},{"title":"Techstars Day2 ~ End - Building Networks, Defining Your Business","date":"2023-02-25","slug":"techstars2","author":{"name":"Kim Dong Hun"},"excerpt":"Building strong networks and defining your startup’s true value were key lessons at Techstars. From leveraging mentorship through genuine advice-seeking to understanding that your real product is the transformation of your customers, these insights highlight the essence of meaningful connections and impactful pitches.","keyword":"startup","content":"\r\nI soon head back to Korea, and I thought that I might never write a post about the Techstars experience after I go to Korea. In this post, I would like to cram all the things I learned from Techstars since Day 2 to its final day.\r\n\r\n## Build network with mentors by asking advice from them\r\n\r\nOn Day 2, John Hill, a post-LinkedIn person, came to the seminar to give us a lecture of how to build networks and find people you need through LinkedIn. Before that, we had been lectured on discovering the 3Ws, Who (Who are you selling to?), What (What are the customers buying - NOT what you are selling!), Why (Why are the customers buying). I do not remember the exact details that were dicsussed in the lecture, but I do remember that we were going through the 3Ws because we had to be prepared to pitch ourselves effectively. In our short pitch (7 ~ 15 seconds), all the 3Ws had to be included to make our pitch clear and informantive.\r\n\r\nThe importance of preparing a clear pitch 3Ws is not limited to us for gaining attention from investors. It has importance because that clear pitch consisted of maybe 3 to 4 sentences is the phrase other people that know us can use to introduce to other people. Imagine we send an email about our business to another person. The person replies that she is not particularly interested but says that she will definitely introduce us to someone else that might be interested. Now she is on the her table writing an email about us to another person who she thinks might be interested in our business. What would happen if we did not provide her a clear informative pitch that she could simply copy and paste on her email? She would have a hard time trying to introduce our business in her own words making the whole process of trying to introducing us to another person cumbersome. She will be easily discouraged to introduce us to other people.\r\n\r\nThe seminar from a post-LinkedIn person's lecture on building network came after the lecture on the importance of creating an informative pitch and how to create a revenue formula. He told us how we could effectively use LinkedIn to easily reach out to someone we could ask guidance from. The one thing that he put great emphasis on while he taught us how we could do it was, \"If you are asking for money in the first meeting of an investor, you are losing\". When you are trying to approach a mentor, be it a investor or an expert, ASK FOR AN ADVICE. When you approach someone asking for an advice, everything shifts since once they have given you a piece of advice you have literally allowed them to get invested in you. Now they take part in your journey.\r\n\r\nHill also remined us that mentors are not paid. They genuinly have no reason to interact with you. However they will mentor you out of altruisum or out of interest in recent business trends. The motivation based on altruism is simply their want to give back what they have earned from their journey. The motivation based on interest in recent business trends is self-explanatory.\r\n\r\nMore to the HOW to use LinkedIn. Hill said that there are special tabs for organizations. Schools have 'alumni' tabs, and companies have 'life' tab or 'people' tab. In fact, these additional tabs especially for companies is actually some feature that companies have to pay for if they would like to have that in their profiles. These 'life' tab or 'people' tab is the best tab you can refer to if you want to know the language or culture of the company because they will definitely have that tab for a reason.\r\n\r\nLast but not least, Hill talked about the importance of watering your network everyday. He says that he personally strictly tightens his network to get a clear network of his first level communications. His directly connected people would be consisted of people he could directly contact with and grab a meeting with the other person that his directly connected person would be in direct connection. Also he recommended posting on LinkedIn since mentors are interested in your journey and they want to see it. Posting under 3 categories was what he also emphasized. His last words of the seminar was \"Build a network before you need it\".\r\n\r\n## Mario analogy for defining your business\r\n\r\nDay 2 ended with John Hill's talk about building networks, and I do not exactly remember what other lectures there wre after Day 2. Later on it was mostly a continuation of meeting mentors and so on. It was till Day 3 that we were lectured, and from then on, each team individually had to meet with mentors and so on. There is not much that I remember from the lectures from then on, but there is one analogy that I remember to this day, and that is what I would like to share in this post.\r\n\r\n![mario analogy](/assets/posts/techstars2/mario.png)\r\n\r\nThe above image is brought from UserOnBoard. This is analogy that Kerty used while talking about how we should pitch ourselves. To anyone familiar to Mario game, when the player eats a mushroom, the player evolves (gets bigger). Kurty taught us the core information that we needed to include when we were to pitch our business in 30 seconds. The pitch had to contain who the customers are, what problem do we sove, what the product is, why should you (investors) care, and optionally a breif founder market fit (ex. \"I previously worked in NASA as an engineer\" would be powerful if your business ahd to do with space engineering). The template provided was, \"There are ~ problems these days. Our solution product is ~. We provide solution to ~, SO THAT OUR CUSTOMERS CAN ~\". The \"SO THAT\" part is the most important part. Like explained in the mario image, what the company is selling is not actually the product itself, but the customers who have evolved after using our product.\r\n\r\nThis mario anoalgy is the reason why I wanted to write this post. It delivers a message that is crucial to a startup. A product is nothing without customers. In fact, in Techstars Day 2, we were constantly reminded the importance of gathering customer testimonials and making the practice of meeting customers one by one daily a habit. The importance of having a customer is also emphasized in a book I am reading now, `Rework`. Seeing customers enjoy your product acts as an important motivation to all team members. Remember, it is not fancy technology or product that you should focus on and put it upfront when you are selling to others. Your real product are the customers that have evolved thanks to your product.\r\n","categories":["Other"],"thumbnail":"/assets/posts/techstars2/mario.png","WIP":false,"data":{"title":"Techstars Day2 ~ End - Building Networks, Defining Your Business","excerpt":"Building strong networks and defining your startup’s true value were key lessons at Techstars. From leveraging mentorship through genuine advice-seeking to understanding that your real product is the transformation of your customers, these insights highlight the essence of meaningful connections and impactful pitches.","date":"2023-02-25","author":{"name":"Kim Dong Hun"},"keyword":"startup","categories":["Other"],"WIP":false,"thumbnail":"/assets/posts/techstars2/mario.png"}},{"title":"Techstars Day1 - Spare just 1 hour and you can build trust within members","date":"2023-01-19","slug":"techstars1","author":{"name":"Kim Dong Hun"},"excerpt":"Team dynamics play a crucial role in startup success. On the first day at Techstars, I learned about building trust through vulnerability, encouraging healthy conflicts, and avoiding harmful triangulation within teams.","keyword":"startup","content":"\n## Spare just 1 hour and you can build trust within members\n\nI am currently in Boston attending an event hosted by techstars. My brother started a startup with a colleauge in UC Berkely last year, and I came to the States to help him build his products with my software engineering skills. His startup was chosen as one of the twelve startups selected by Techstars for the evennt. Twelve companies doing their business in the crypto world gathered in Boston to get mentored by Techstars.\n\nOn the first day of the Techstars event (2023-01-19), all attendants sat in a room to listen to a lecture on teamwork. I have never attended to any MBA classes, but one could say the whole lecture was quite similar to classes provided in MBA programs. We were taught about some theories and strategies to manage a team. But I believe this lecture had more vitality than a normal MBA class in that all participants were CEOs of their startups and thus were eager to get the most out of the lecture.\n\nAt first, the whole lecture seemed boring. As a student who had listened to business management classes back in Seoul National University, all the things that were taught by the lecturer seemed too obvious. The main topic was about functions of a team, what differentiates a good team and a bad team. However, as the lecture progressed, I felt there were some parts worth notetaking.\n\n![Boston Techstars](/assets/posts/techstars1/meeting.png)\n\n### Develop deep relations when a newcomer joins\n\nIn the first part of the lecture, the lecturer asked each one of us to answer to three questions.\n\n1. Where did you grow up?\n2. How many siblings do you have? (Are you the youngest or the oldest?)\n3. Did you have any unique challenges during your childhood. And if so, how did that challenge affect you in your life.\n\nThe first two questions are just simple questions, but the last one is something that requires some thought and some time to think of. Taking turns, people talked about their unique challenges. Some talked about how their unique challenges made eventually made them do a startup, some just talked about their difficult childhood, and some talked about their coutnry's poor situation. Each person took a significant amount of time talking of their own stories.\n\nIt took approximately one hour for all the 30 people to share all their stories. After everyone spoke, the lecturer asked us if we felt more acquainted with one another. And it sure did. He was lecturing the importance of opening up your vulnerability to your team members, and he told us that by going through this process, team workers can get to know each other better and be reliable to each other. It just took 1 hour for 30 people to answer the questions. How long will it take for a team of 10 people to go through this process? Just sparing only a few minutes will be enough to make everyone feel deeply connected to each other.\n\nThis was the part I realized the importance of having an off-site occasion whenever a newcomer joins a group. No matter what kind of work the team members are doing, if the team has decided to embrace a new team member, everyone should take a leave for their work at least for 1 hour to openly talk with the newcomer and open themselves up to vulnerability. No excuses whatsoever. The newcomer is a priceless and thankful person to have decided to join the risky startup journey together with the team, and thus, the team should pay their utmost respect to the newcomer. Remove all the walls you may have in normal situations and open yourself to the newcomer. The best way to form the atmostphere of being connected is to ask each participant about their childhood difficulties.\n\n### Boring team meetings is a meeting where there are no conflicts\n\nNext, we were lectured about conflicts. Some of the lecture notes were obvios, but what struck me was the sentence, \"Boring team meetings is a meeting where there are no conflicts.\" The lecturer also added that if a team meeting ended without having any conflicts is a bad sign. The fact that there are some conflicts regarding their project (personal conflicts are disregarded here).\n\nI personally reflected on myself a lot on this subject. When I am in a situation of managing a team, I tend to be some sort of a manager-who-knows-all (As a type of person who plans everything, I plan everything before doing some project). When somebody objects to what I have planned for a project, it is quite difficult for me to discuss that matter in a leisurely manner. To other team members, I believe I might seem as though I am a stubborn boss when having a conflict where I am in charge of the project.\n\nThe lecturer commented about the importance of creating an environment where an employee can speak up and raise objections. If your attitude makes the team have a hard time raising a healthy conflict in the first place, it is important to contain yourself for the team. The lecturer talked about one CEO who practiced to contain himself whenever an employee raises an objection. The reason he practices it is because he knows well that once he takes immediate respondence to that opposite opinion, the person who raised an objection will fall down immediately and seldomly raise any objections in the future. The lecturer commented that to remedy the situtation, the CEO came up with an idea to always put a person who would always replies with a 'thank you for voicing your opinion' to the person who raised the objection in a meeting. When I happen to be leading a team, I believe I should also think of my own ways to make myself and the team openly object to any ideas they think are wrong or could be done better.\n\nDelving in to the topic about reacting to conflicts, the lecturer said that how you react to conflicts are predominantly learned from your caregivers. No matter what environment you were reared from, how you deal with conflicts does not change much from how you were learned to deal with conflicts from your caregivers. How you react to conflicts with other people has to do with how your parents fight each other and later patch up.\n\nThis also made me reflect upon myself again. Mostly my mother would be quiet when there is a fight between her and father. Similar to her, when I am being the one being scolded especially in personal conflicts, I tend to stay quiet. I cannot come up with any sentences to respond to that scold unless it is something utterly nonsense. When emotions rush, my sentence get loose. When some work does not seem to go well, I feel somewhat irritated and add emotion to my words. I believe this is somewhat similar to how my mother expresses anger in some occasions. She sometimes express her discomfort in my father's actions with emotion full of irritation.\n\n### Beware Triangulation.\n\n![Triangulation](/assets/posts/techstars1/triangulation.png)\n\nThe last memorable topic that the lecturer taught about was 'Triangulation'. 'Triangulation' is a term the lecturer himself coined up. To briefly explain what it is, it is a situation where B tells C the problems he/she has with A instead of having direct conversations with A. The lecturer said that this is one of the signal that a team is not operating well. It seems very obvious, but coming to think of it, it happens rather often in many cases in life, even mine also. The main reason why Triangulation is harmful to a startup especially is because it means that team members are avoiding conflicts. They are just being bystanders to a dying company. This can also lead to decreased commitment among team members since avoiding direct conflict with another person means that people are trying separate in groups. What can be worse than a small group of people in a startup dividing into groups?\n\nThese were the things I wanted to share for the first day of Techstars. It was a semi-MBA class but I believe it was worth it. I actually learned a lot and had some time to reflect on myself.\n","categories":["Other"],"thumbnail":"/assets/posts/techstars1/meeting.png","WIP":false,"data":{"title":"Techstars Day1 - Spare just 1 hour and you can build trust within members","excerpt":"Team dynamics play a crucial role in startup success. On the first day at Techstars, I learned about building trust through vulnerability, encouraging healthy conflicts, and avoiding harmful triangulation within teams.","date":"2023-01-19","author":{"name":"Kim Dong Hun"},"keyword":"startup","categories":["Other"],"WIP":false,"thumbnail":"/assets/posts/techstars1/meeting.png"}}],"postCategories":["Computer Graphics","Machine Learning","Other"],"allProjects":[{"title":"Visualizing a Nation's Blood Donation and Demand","date":"2024-12-21","slug":"korea_blood_donation","excerpt":"This project visualizes South Korea's blood donation system, analyzing supply, demand, and the impact of promotional events. It reveals seasonal imbalances and the significant effect of COVID-19 on donation trends.","keyword":"Data Visualization","content":"\n[Visit Website →](https://hunkim98.github.io/korea-blood-donation/)\n\n![Demo](/assets/project/korea_blood_donation/thumbnail.gif)\n\nMany have reported that the blood donation rate is decreasing. In January 2024, the U.S. Red Cross announced that the current status of blood donation is a emergency blood shortage, explaining that the number of blood donors hit all-time low for the past 20 years. \n\nTo confirm whether such a problem exists, I decided to delve into this issue. Initially, I tried to study the blood donation status of the U.S., but due to multiple parties controlling the blood donation system in the U.S., I instead chose another country that had a centralized blood donation system. I ended up choosing South Korea since it well-documented their blood donation system and had a centralized entity controlling the blood donation system.\n\n### Data Collection\n\nThe data was collected from KOSIS(KOrean Statistical Information Service). I did this project with a class mate, and we gathered the number of blood donors, the number of blood donation, and the number of blood donation centers. \n\nWhile gathering data for blood donation supply was relatively easy, there was no definitive data for how the donated blood was used. We ended up extrapolating the data using a proxy data. We collected the number of patients, the number of surgeries, and the number of blood transfusions from KOSIS. Through calculating the blood used in hospitals each year, we were able to estimate the demand for blood. \n\nIn addition to blood donation supply (donors) and demand (hospitals), we also collected events the Korean Red Cross held to promote blood donation in the country. In doing so, we hoped that we could find a correlation between the promotional events and the number of blood donors.\n\n![Data Heatmap](/assets/project/korea_blood_donation/heatmap.png)\n\n### Strategy \n\nWith all our data, we decided to create an effective visual representation of the blood donation supply, blood usage demand, and the promotional events. We found that there is an uneven distribution of blood donation frequencies depending on the month and decided to find a way to effectively visualize the discrepancy. We strategized to focus on the ratio of blood donation and blood usage.\n\nAfter multiple experimentation, we found that balancer-looking shape was effective to show how imbalanced a blood donation supply and demand are. In our visualized blood donation supply and demand, the left thick line represents the blood donation, and the right colorless thin line represented the blood usage. The thicker the left area, the more blood donation there was compared to the blood usage. We additionally used color and angle to represent how much the ratio is off.\n\n![Visualization Strategy](/assets/project/korea_blood_donation/tokenization.png)\n\n### Findings\n\nThrough visualizing, we could discover that South Korea especially has a healthy ratio of blood donation and blood usage especially near May, and this seems to be due to the summer season and the frequency of promotional events held by the Red Cross. However, the ratio of blood donation and blood usage is not as healthy in the winter season. \n\nInterestingly, after March 2020, the time when COVID-19 started to spread in Korea, the ratio of blood donation and blood usage has decreased significantly. From this, we can know that the COVID-19 pandemic has had a significant impact on the blood donation system in South Korea.","categories":["Data Visualization"],"thumbnail":"/assets/project/korea_blood_donation/thumbnail.gif","WIP":false,"data":{"title":"Visualizing a Nation's Blood Donation and Demand","excerpt":"This project visualizes South Korea's blood donation system, analyzing supply, demand, and the impact of promotional events. It reveals seasonal imbalances and the significant effect of COVID-19 on donation trends.","date":"2024-12-21","author":{"name":"Kim Dong Hun"},"keyword":"Data Visualization","categories":["Data Visualization"],"WIP":false,"thumbnail":"/assets/project/korea_blood_donation/thumbnail.gif"}},{"title":"Dotting GenAI: Pixel Art Editor With Generative AI","date":"2023-04-11","slug":"dotting_genai","excerpt":"Dotting GenAI explores the integration of generative AI into pixel art editing, offering tools for direct modification of AI-generated images. The project was tested with design experts and recognized as a top 16 project at Primer's 2023 Hackathon.","keyword":"Generative AI","content":"\n<!-- [Hackathon Presentation Video →](https://www.youtube.com/watch?v=nKYJNuTxfTs) -->\n\n![Demo](/assets/project/dotting_genai/demo.gif)\n\nGenerative AI is everyday nowadays. Everyone in town are hype about it. However, while generative features such as text-to-image AI are promising, there is one limitation: it is not easy to modify the generated contents as you intended. Especially for text-to-image generation tasks, delivering your intention of how you want to modify an image only through text prompts is a nightmare.\n\nFor mass adoption of generative AI, allowing users to easily modify generated image output as they intend to do so is crucial. In fact, in most scenarios, the tools that users want to use for modifying images are changing a color of an area or removing a specific region out of the image. To achieve that goal, generative AI services should be integrated into an editor that provides additional modification tools. At the moment, however, there exists no definitive software design principles on how to merge the two distinct experiences: generating and editing.\n\nTo test the methods of integrating Generative AI into editors myself, I created a pixel-art editor with Generative AI features and named it Dotting GenAI. I also used this project to participate in a 2023 Hackathon hosted by the a Korean Venture Capital, Primer. Dotting GenAI was awarded as one of the top 16 projects in the hackathon. It was created with an opensource project that I have started to provide React developers a pixel art editor, `Dotting`.\n\n![Editor screen](/assets/project/dotting_genai/home.png)\n\n### Chat Agent for Generating Pixel-art\n\nDotting GenAI allows users to interact with the generative AI through a chat interface positioned on the right of the screen. The user can ask the generative AI to generate an image, and the generative AI will generate an image based on the user's request. The generated images will return as replies in the chat log. The user can place the generated image on the pixel art canvas and then modify the generated image with the editor tools provided by Dotting GenAI.\n\n### User Test\n\nAfter the development of the service, I conducted a user test with people who specialize in desing to figure out 1) whether my software design approach was effective, and 2) whether the generative AI was useful for them. The user test was conducted with 12 design experts, and the results showed some interesting insights.\n\nMost users thought the experience of being able to modify the generated images directly in the editor as comfortable. However, they had many worries about using generative AI in their work. Some reported that although generated images do encourage untried styles, they become a burden when the user wants to create a specific image they have in mind. Some also reported that they rather wanted a generative AI that learns from their own styles, rather than some random generated images based on prompts. People more interested in the study may download the [PDF file](/assets/project/dotting_genai/dissertation.pdf) for more details.\n\n![Presentation at Primer Hackathon](/assets/project/dotting_genai/presentation.png)\n","categories":["Machine Learning","Product Design","Web Development"],"thumbnail":"/assets/project/dotting_genai/demo.gif","WIP":false,"data":{"title":"Dotting GenAI: Pixel Art Editor With Generative AI","excerpt":"Dotting GenAI explores the integration of generative AI into pixel art editing, offering tools for direct modification of AI-generated images. The project was tested with design experts and recognized as a top 16 project at Primer's 2023 Hackathon.","startDate":"2023-03-22","date":"2023-04-11","author":{"name":"Kim Dong Hun"},"keyword":"Generative AI","categories":["Machine Learning","Product Design","Web Development"],"thumbnail":"/assets/project/dotting_genai/demo.gif","WIP":false}},{"title":"Cryptogalaxy: Visualizing cypto markets","date":"2022-12-11","slug":"cryptogalaxy","excerpt":"CryptoGalaxy visualizes the crypto market through a dynamic, space-themed design. It connects galaxy components like the sun, planets, and spaceships to market indicators, providing an interactive, real-time representation of crypto trends.","keyword":"crypto","content":"\n[Visit Website →](https://hunkim98.github.io/cryptogalaxy/)\n\n![CryptoGalaxy Logo](/assets/project/cryptogalaxy/logo.png)\n\n![Demo](/assets/project/cryptogalaxy/demo.gif)\n\nCryptoGalaxy is an attempt that visualizes the Crypto Market in a fun and interactive way.\n\nTraditionally, markets are represented as a table of numbers, full of jargon. CryptoGalaxy aims to change that by providing a fun and interactive way to visualize the Crypto Market.\n\n![Crypto Planets Orbiting BTC Sun](/assets/project/cryptogalaxy/screen.png)\n\nThe conduits that convey information in CryptoGalaxy are the sun, planet, and spaceships. The Sun represents Bitcoin(BTC), and planets represent other coins such as Ethereum(ETH), Dogecoin(DOGE). The sentiments regarding a specific coin(planet) was shown with the in-and-outs of the spaceships.\n\nThe indicators used for visualization were Moving Average Increase Rate, Market Capital, Relative Strength Index(RSI), Money Flow Index(MFI), and Correlation Coefficient of the trend similarity between BTC and a specific coin. \n\n\n### Sun \n\nIn CryptoGalaxy, the sun represents the Bitcoin as it is the first crypto currency to have appeared. To a sun, the most important feature is its brightness, and its brightness was connected with the Moving Average Increase Reat of the Bitcoin. \n\n### Planets\n\nThe planets that orbit around the sun represent other coins such as Ethereum(ETH), Dogecoin(DOGE). The size of the planet was connected to the Market Capital of the coin. Different from the sun, the planets have more features that can be connected with the indicators. The planet's orbit speed and ice-age degree were the features that were connected with the indicators. Similar to how the sun's brightness was connected to the Moving Average Increase Rate of the Bitcoin, the orbit speed of the planet was connected to the Moving Average Increase Rate of the coin. The ice-age degree was connected to the MFI of the coin.\n\n### Spaceships\n\nThe most dynamic component, the spaceships, represent the main activity happening to a specific coin. The in-and-outs represent the buying and selling of the coin. Since RSI is an indicator that shows the sentiment of the market, the in-and-outs of the spaceships were connected to the RSI of the coin.\n\nThe project was built using Typescript and HTML Canvas. The data was fetched from Dunamu's Upbit API in real time. The project was deployed using Github Pages and later exhibited in Seoul National University's 2022 Visual Design Graduation Exhibition.\n\n![Installation Viewed from Side](/assets/project/cryptogalaxy/front.jpg)\n","categories":["Data Visualization","Computer Graphics"],"thumbnail":"/assets/project/cryptogalaxy/demo.gif","WIP":false,"data":{"title":"Cryptogalaxy: Visualizing cypto markets","excerpt":"CryptoGalaxy visualizes the crypto market through a dynamic, space-themed design. It connects galaxy components like the sun, planets, and spaceships to market indicators, providing an interactive, real-time representation of crypto trends.","startDate":"2022-09-01","date":"2022-12-11","author":{"name":"Kim Dong Hun"},"keyword":"crypto","categories":["Data Visualization","Computer Graphics"],"thumbnail":"/assets/project/cryptogalaxy/demo.gif","WIP":false}},{"title":"Webnovelr: Text editor service for web novelers","date":"2022-12-11","slug":"webnovelr","excerpt":"Webnovelr is a text editor prototype designed for web novel writers, featuring a 'Jopan'-based layout tool to help authors optimize reading flow and improve the webnovel writing experience.","keyword":"webnovel","content":"\n[View Website →](https://hunkim98.github.io/webnovelr/)\n\nThe webnovel ecosystem is comprised of various writers and readers. However, despite the ecosystem being a media where many people come and go, the truth is that there are many visual aspects lacking considerations of the participants. Webnovelr is a project that attempts to better the ecosystem of webnovels.\n\nWebnovels are usually served by only a few platforms in Korea. The representative platforms are Kakao Page and Munpia. Webnovel writers send their literature to these platforms. Each platform have different layouts, thus showing the same literature in a different way. Writers name these differences as ‘Jopan’, which is known to affect how readers read the literature.\n\nTo writers, ‘the reader’s speed of reading’ is as important as the originality of the webnovel. If a tense scene does not fit in one page but continues on to the next page, there is a possibility where readers can feel bored of the webnovel. Despite these concerns, there exists no editor that allows webnovel writers to keep track of the reader’s speed of writing. To make the writing experience easier for webnovel writers, Webnovelr presents a ‘Jopan’ based editor.\n\n![Home Screen](/assets/project/webnovelr/home.png)\n\n![Text Edit Page](/assets/project/webnovelr/edit.png)\n\n![Keyword Search UI](/assets/project/webnovelr/keyword_search.png)\n\n![Character Edit Page](/assets/project/webnovelr/character_list.png)\n\n![Exhibition at Seoul National University](/assets/project/webnovelr/installation.jpg)\n","categories":["Data Visualization","Web Development","Product Design"],"thumbnail":"/assets/project/webnovelr/screen.png","WIP":false,"data":{"title":"Webnovelr: Text editor service for web novelers","excerpt":"Webnovelr is a text editor prototype designed for web novel writers, featuring a 'Jopan'-based layout tool to help authors optimize reading flow and improve the webnovel writing experience.","startDate":"2022-09-01","date":"2022-12-11","author":{"name":"Kim Dong Hun"},"keyword":"webnovel","categories":["Data Visualization","Web Development","Product Design"],"thumbnail":"/assets/project/webnovelr/screen.png","WIP":false}},{"title":"Visualize my friend's social network","date":"2022-12-08","slug":"linklink","excerpt":"LinkLink is a social networking app designed to simplify the 'friend of a friend' process. It visualizes connections and offers evaluation features to find reliable companions for specific purposes like startups.","keyword":"social network","content":"\n[Project Wiki →](https://github.com/swsnu/swppfall2022-team9/wiki/Requirements-and-Specification)\n\n![Demo](/assets/project/linklink/demo.gif)\n\nWhen people start a startup, they start by finding suitable people to work with. In most cases, they reach out to their friends and ask them to join the startup. If the friends say that they are unavailable, then the person ask them to introduce their friends. This is because getting referrals from trusted relationships is the best way one can gather suitable companions.\n\nThe case described above happens very often in the startup world, and I have experienced the process myself. Unfortunately, the problem is that this process is very inefficient. You have to meet each of your friends to get information about their acquaintances, and then ask them to introduce you some friends they think might be a good fit with you.\n\nCan this whole process be simplified? Why not create an application that directly shows my friend's network?. That is why I came up with LinkLink, a social networking application just for that purpose. There are many social networking applications out there such as LinkedIn. However the problem is that their purpose is too general, and thus not suitable for special purposes. LinkLink focuses on the \"friend of a friend\" process, where one can view the qualities of the friends of their friends, and then ask the intermediary friend to introduce him or her to oneself.\n\n![Friend Network Visualization](/assets/project/linklink/home.png)\n\n### Previous Works\n\nLinkedIn previously did a [similar project](https://blog.linkedin.com/2011/01/24/linkedin-inmaps) in 2011. It visualized all the networks of the acquaintances you had. However, the problem was that the visualization was too complex, and thus not very useful. There was too much information in the visualized network resulting in an ununderstandable mess. \n\nLinkLink solves this visual mess by putting a limit to how many direct friend connections one can have (one can only have a maximum of 15 direct friends). This benefits the service in two ways: 1) The service is more reliable since one has to carefully choose who to connect with, resulting in a more reliable network, 2) The user is able to understand the network better since there are less connections to look at.\n\n![Personal Profile UI](/assets/project/linklink/profile.png)\n\n![Evaluate Friends](/assets/project/linklink/evaluate.png)\n\n\n### Features\n\nSince the main purpose of the application was to provide users with reliable networks, our team devised evaluation features for users to evaluate their directly connected friends. This allows users to not only judge the fitness of another person based on their skillsets but also their character. When one finally decides they want to connect with that person, they can chat with the person directly or ask their intermediary friend to introduce them to the person.\n\n![Chat](/assets/project/linklink/chat.png)\n","categories":["Data Visualization","Web Development","Product Design"],"thumbnail":"/assets/project/linklink/demo.gif","WIP":false,"data":{"title":"Visualize my friend's social network","excerpt":"LinkLink is a social networking app designed to simplify the 'friend of a friend' process. It visualizes connections and offers evaluation features to find reliable companions for specific purposes like startups.","startDate":"2022-09-01","date":"2022-12-08","author":{"name":"Kim Dong Hun"},"keyword":"social network","categories":["Data Visualization","Web Development","Product Design"],"thumbnail":"/assets/project/linklink/demo.gif","WIP":false}},{"title":"Toonie: Real-time Collaborative image review editor","date":"2022-08-19","slug":"toonie","excerpt":"Toonie is a real-time CRDT-based collaborative image review editor designed for streamlined image feedback. It introduces optimized techniques for multiplayer whiteboards, separating real-time interactions from committed actions for enhanced performance.","keyword":"collaboration","content":"\nCollaborative editing is a rising star in software development. Now, people not only want smart editors, but they also want an editor that allows real-time collaboration with other people. Collaborating with other people is a great way to increase productivity and get feedback on your work. The COVID-19 pandemic familiarized people with the concept of collaborative editing, and now, many people are using collaborative editors in their daily lives.\n\n### Difficulties in implementing multiplayer editors\n\nHowever, implementing a multiplayer aspect into an editor is not an easy task. First, one should have basic knowledge of how to manage multiple actions from multiple agents. What happens if two people try to edit the same part of the document at the same time? What happens if one person deletes a part of the document while the other person is editing the same part? These are the questions that one should be able to answer when they want to implement a collaborative editor. \n\nSecond, one should have to decide what features shall be communicated between users, and how their concurrent actions do not cause overhead. Relaying data between users can be costly and unreliable in some cases, so one should carefully design the data flow between users in a way that it does not harm user experience.\n\n### Building a whiteboard multiplayer editor\n\nToonie is a real-time CRDT-based collaborative image review editor that I created to research optimization techniques for implementing CRDT mechanisms into whiteboards. In Toonie, users can review on images uploaded to the service together by sharing a URL to another person. The reason I chose an image review domain specifically is because most collaborative editors out there are for general purposes. Thus, I wanted to create a more narrow focused whiteboard where designers and marketers could review on images together by drawing sketches on the images in an online meeting setting.\n\n![Editing Scene](/assets/project/toonie/edit.png)\n\nToonie wass built on top of the CRDT-based collaborative SDK called [Yorkie](https://github.com/yorkie-team/yorkie) which is an opensource document store for building collaborative applications created by Naver Alto TF. While contributing to the opensource project, I created Toonie for demonstrating the capabilities of Yorkie.\n\n### How Toonie works\n\nWhen creating a multiplayer whiteboard, one should distinguish between actions that need its commit order to be preserved and actions that do not need its commit order to be preserved. This is because in multiplayer operations, actions that needs its order to be preserved consume much computer resources during communication, which can eventually result in lags. \n\nIn Yorkie, actions that do not need its order to be preserved were managed as `Presence`, and those that needed its order to be preserved (= that is in need of management through CRDT algorithms) were managed as `Data`. In a whiteboard application, the real-time action (interactions that the user commits to the whiteboard while their mouse is down) do not really need to be communcated with its order preserved since their actions are not finished, and thus can be managed as `Presence`. However, the committed action, which is a finished action of a user (mouse up after mouse down) should have its order preserved since it is a finished action. Those were managed as `Data`.\n\n![User interaction recorded as presenc](/assets/project/toonie/soc_step1.png)\n\n![User interaction finishes and presence is reset](/assets/project/toonie/soc_step2.png)\n\n![Presence modifications are transferred to data](/assets/project/toonie/soc_step3.png)\n\n### Methods of Optimization\n\nSeparation of concerns was used for implementing the distinction between `Presence` and `Data`, and thus multiple layers (presence canvas and data canvas) were designed for the whiteboard application. A user's real-time unfinished interactions (whiteboard modifications that occur while the user's mouse is pressed) were drawn in the `Presence` canvas. And as soon as the user's interaction finishes (user finishes modification by mouseup), then the `Presence` canvas data is reset and the modification data is sent to the `Data` Canvas. By doing so, I was able to design a software design approach for optimizing collaborative whiteboards.\n","categories":["Web Development","Computer Graphics"],"thumbnail":"/assets/project/toonie/thumbnail.png","WIP":false,"data":{"title":"Toonie: Real-time Collaborative image review editor","excerpt":"Toonie is a real-time CRDT-based collaborative image review editor designed for streamlined image feedback. It introduces optimized techniques for multiplayer whiteboards, separating real-time interactions from committed actions for enhanced performance.","startDate":"2022-06-27","date":"2022-08-19","author":{"name":"Kim Dong Hun"},"keyword":"collaboration","categories":["Web Development","Computer Graphics"],"thumbnail":"/assets/project/toonie/thumbnail.png","WIP":false}},{"title":"Persona Personality: Discover the personality masks of my friends!","date":"2021-06-01","slug":"personapersonality","excerpt":"Persona Personality is an Enneagram-based personality test designed to explore the multifaceted personalities of acquaintances. It helps users understand others while challenging the notion of static personality definitions.","keyword":"enneagram","content":"\n<!-- [View Website →](https://www.personapersonality.com/) -->\n\nIn 2021, young people (mostly 20s) in Korea were crazy with MBTI (Myers-Briggs Type Indicator). MBTI is a personality test that divides human personalities into 16 types. It figures out one's personality by discovering one's preference in four categories: Extroversion(E) vs. Introversion(I), Sensing(S) vs. Intuition(N), Thinking(T) vs. Feeling(F), and Judging(J) vs. Perceiving(P). In each of the four categories, one can be either one of the two types.\n\nThe simple and easy-to-understand MBTI test captivated many young Korean people thanks to its quick and intuitive results. Nowadays, more and more MBTI test websites are created daily, and their contents are constantly consumed by many young people.\n\n### MBTI being misued\n\nHowever, I noticed a pattern of misuse of MBTI results among many young people. MBTI is known as one of the tests that has its basis in 'positive psychology' - it was intended to enable people to see positive parts of oneself. Unfortunately, young people started to treat MBTI as an ultimate truth and started to use it as a tool to define limitations on oneself. \n\nLet me give you one example misusage pattern. Let us say one person found out that he/she is classified as I (Introversion) rather than E (Extroversion) in MBTI's first category. Then the person would think that he/she is not good at socializing with others and use that result to explain to others why he/she cannot do something that has to do with confronting others in public. As such, MBTI results started to be consumed as an excuse or justifications to one's limitations.\n\nThe main reason to this phenomenon was because most MBTI web-based tests aim to to give a single interpretation on one's personality. I found this faulty, I felt the need for a new test that allows one to realize that one's personality is in fact multi-faceted and something dynamic.\n\nI embarked on creating a new personality test that figures out the personality of one's acquaintances rather than oneself. I devised it in a way that the recepient of the personality result receives multiple interpretations of his/her own personality from others, and realize that one's personality cannot be defined into a single statement.\n\n![9 Mask Results in Personapersonality](/assets/project/personapersonality/masks.png)\n\n### Building the application, Enneagram\n\nPersonapersonality is the application that I created for this purpose. Its pyshcology test basis is Enneagram, which is a personality trait theory that divides human personalities into 9 types. \n\nThe reason Enneagram was chosen instead of MBTI was because 1) MBTI had a limited explanation on one's personality while Enneagram explained one's personality holistically, and 2) I wanted to prevent people from interpreting the results in the MBTI point of view. The questions asked in the application were based on the user's acquaintance, and questions such as \"What do you think your (acquaintance's name) would do when he has to lead a classroom conference?\". \n\nAfter answering custom curated 15 questions, one could get the enneagram result of the other person. The personality result was presented as a form of a mask, which is a visual representation of the personality type in Personapersonality. This was intended since in my point of view, one's personality can be understood differently based on how the person interacts with that person. Thus, the name 'Persona' personality was given to the application.\n\n![Question Screen](/assets/project/personapersonality/question.png)\n\n![Mask Result Screen](/assets/project/personapersonality/mask_result.png)\n\n\n### Features\n\nTips on maintaining a good relationship with people with such personality were provided alongside the explanation of the other person's personality, and this was because the application itself was intended to be used as a tool to understand others better. \n\nSince the initial goal was to preven people from understanding one's personality as one single truth, I also provided a data visualization of the analyzed results, where one could check the percentage of each personality type that was given to the person. This was to show that one's personality is not a single truth, but rather a combination of multiple personalities.\n\n![Personality Mask Result Analysis](/assets/project/personapersonality/result_analysis.png)\n\nAs of today (2023.07.18), 204495 people have used the application to figure out the personality of their acquaintances.\n","categories":["Web Development"],"thumbnail":"/assets/project/personapersonality/thumbnail.png","WIP":false,"data":{"title":"Persona Personality: Discover the personality masks of my friends!","excerpt":"Persona Personality is an Enneagram-based personality test designed to explore the multifaceted personalities of acquaintances. It helps users understand others while challenging the notion of static personality definitions.","startDate":"2020-09-01","date":"2021-06-01","author":{"name":"Kim Dong Hun"},"keyword":"enneagram","categories":["Web Development"],"thumbnail":"/assets/project/personapersonality/thumbnail.png","WIP":false}}],"projectCategories":["Data Visualization","Machine Learning","Product Design","Web Development","Computer Graphics"]},"__N_SSG":true}