<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Pixel_horizons: Procedural Generation of Pixel Art Landscapes using LLM</title><meta name="description" content="This project attempts to leverage LLM&#x27;s ability to contextually understand what is happening in the game world and generate adequate text descriptions to create pixel art landscapes."/><meta name="title" content="Pixel_horizons: Procedural Generation of Pixel Art Landscapes using LLM"/><meta property="og:image" content="https://donghunkim.dev/assets/project/llm_procedural_generation/thumbnail.gif"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"/><link rel="manifest" href="/favicon/site.webmanifest"/><link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#000000"/><meta name="msapplication-TileColor" content="#000000"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#000"/><meta name="og:sitename" content="Donghun Kim"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><meta name="google-site-verification" content="l9pNikVAOmXekB00LXYnclf9f_nyVIIjDvu4s2DdYtQ"/><meta name="next-head-count" content="17"/><link rel="preload" href="/_next/static/media/5d7fcf765059a344-s.p.otf" as="font" type="font/otf" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/ac7c9f14c7799d15-s.p.otf" as="font" type="font/otf" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/535c141938fc81c3-s.p.otf" as="font" type="font/otf" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/e11418ac562b8ac1-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/a955ec04d7fe9324.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/a955ec04d7fe9324.css" crossorigin="" data-n-g=""/><link rel="preload" href="/_next/static/css/92b5d4034cf97289.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/92b5d4034cf97289.css" crossorigin="" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-66d32731bdd20e83.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-5d77cecfe18a5947.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-e35f58c0210c308e.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/fb7d5399-77a75513571cc99c.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/317522db-a1e300886af6caee.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/336-0626ea9457c579cc.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/619-9195e5b60e53e070.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/634-0a3f0a1b4521217b.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/projects/%5Bslug%5D-bf4878c317f515c4.js" defer="" crossorigin=""></script><script src="/_next/static/MfYEUNrk9JLrhJ_NpIfgP/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/MfYEUNrk9JLrhJ_NpIfgP/_ssgManifest.js" defer="" crossorigin=""></script></head><body class="bg-[black]"><div id="__next"><style data-mantine-styles="classes">@media (max-width: 35.99375em) {.mantine-visible-from-xs {display: none !important;}}@media (min-width: 36em) {.mantine-hidden-from-xs {display: none !important;}}@media (max-width: 47.99375em) {.mantine-visible-from-sm {display: none !important;}}@media (min-width: 48em) {.mantine-hidden-from-sm {display: none !important;}}@media (max-width: 61.99375em) {.mantine-visible-from-md {display: none !important;}}@media (min-width: 62em) {.mantine-hidden-from-md {display: none !important;}}@media (max-width: 74.99375em) {.mantine-visible-from-lg {display: none !important;}}@media (min-width: 75em) {.mantine-hidden-from-lg {display: none !important;}}@media (max-width: 87.99375em) {.mantine-visible-from-xl {display: none !important;}}@media (min-width: 88em) {.mantine-hidden-from-xl {display: none !important;}}</style><main class="__variable_56c696 __variable_3a0388"><div class="" style="position:fixed;top:0;left:0;height:2px;background:transparent;z-index:99999999999;width:100%"><div class="" style="height:100%;background:rgba(255,255,255,0.8);transition:all 500ms ease;width:0%"><div style="box-shadow:0 0 10px rgba(255,255,255,0.8), 0 0 10px rgba(255,255,255,0.8);width:5%;opacity:1;position:absolute;height:100%;transition:all 500ms ease;transform:rotate(2deg) translate(0px, -2px);left:-10rem"></div></div></div><div style="transition:all 0.3s ease-in-out;backdrop-filter:blur(10px);align-items:center;justify-content:space-between;padding-left:calc(1.25rem * var(--mantine-scale));padding-right:calc(1.25rem * var(--mantine-scale));background:rgba(0,0,0,0.2);width:100%;height:calc(3.5rem * var(--mantine-scale));position:fixed" class="z-50 text-white m_8bffd616 mantine-Flex-root __m__-R2nm"><div style="transform:rotate(0deg);transition:transform 500ms" class="cursor-pointer transition-all"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="tabler-icon tabler-icon-arrow-up "><path d="M12 5l0 14"></path><path d="M18 11l-6 -6"></path><path d="M6 11l6 -6"></path></svg></div><style data-mantine-styles="inline">.__m__-Rbbanm{position:relative;}@media(min-width: 62em){.__m__-Rbbanm{position:absolute;left:50%;}}</style><div style="white-space:nowrap;transform:translate(-50%, 0);gap:var(--mantine-spacing-sm)" class="m_8bffd616 mantine-Flex-root __m__-R1banm __m__-Rbbanm"><style data-mantine-styles="inline">.__m__-R2rbanm{max-width:calc(-6.25rem * var(--mantine-scale));}@media(min-width: 62em){.__m__-R2rbanm{max-width:none;}}</style><p class="mantine-focus-auto truncate m_b6d8b162 mantine-Text-root __m__-R2rbanm"><span class="mantine-focus-auto w-fit m_b6d8b162 mantine-Text-root"><span class="mantine-focus-auto font-sans font-medium m_b6d8b162 mantine-Text-root">Machine Learning</span><span style="margin-left:calc(0.3125rem * var(--mantine-scale));margin-right:calc(0.3125rem * var(--mantine-scale))" class="mantine-focus-auto m_b6d8b162 mantine-Text-root">|</span><span class="mantine-focus-auto font-sans font-normal m_b6d8b162 mantine-Text-root">Pixel_horizons: Procedural Generation of Pixel Art Landscapes using LLM</span></span></p></div><style data-mantine-styles="inline">.__m__-R1ranm{display:none;}@media(min-width: 62em){.__m__-R1ranm{display:block;}}</style><div class="__m__-R1ranm"><p class="mantine-focus-auto font-sans m_b6d8b162 mantine-Text-root">2025</p></div></div><div style="height:300px;background-image:url(/assets/background/noise30.png)" class="relative"><div style="height:150px;width:100%;background-image:linear-gradient(to top, rgba(0,0,0,1), rgba(0,0,0,0));position:absolute;top:calc(9.375rem * var(--mantine-scale));left:0rem" class=""></div></div><div class="relative min-h-screen text-[#fff] overflow-x-hidden"><main><div class="container mx-auto px-5 max-w-5xl"><article class="mb-32"><div class="max-w-3xl mx-auto"><div style="gap:var(--mantine-spacing-lg);flex-direction:column" class="mb-6 text-lg m_8bffd616 mantine-Flex-root __m__-R5knm"><p style="--text-fz:calc(2.5rem * var(--mantine-scale));--text-lh:40px;line-height:1.2em;letter-spacing:1px" class="mantine-focus-auto font-tiempos m_b6d8b162 mantine-Text-root">Pixel_horizons: Procedural Generation of Pixel Art Landscapes using LLM</p><p style="margin-top:calc(0.5rem * var(--mantine-scale));opacity:0.5" class="mantine-focus-auto font-tiempos font-thin m_b6d8b162 mantine-Text-root"><span class="mantine-focus-auto font-sans m_b6d8b162 mantine-Text-root">Posted on:</span><span style="margin-left:calc(0.3125rem * var(--mantine-scale))" class="mantine-focus-auto font-sans m_b6d8b162 mantine-Text-root">2025-01-26</span></p><p class="mantine-focus-auto font-sans font-thin m_b6d8b162 mantine-Text-root">Category:<!-- --> <span class="mantine-focus-auto font-sans m_b6d8b162 mantine-Text-root">Machine Learning<!-- --> </span></p></div></div><div class="max-w-3xl mx-auto"><div class="markdown-styles_markdown__MNzbo"><p><img src="/assets/project/llm_procedural_generation/thumbnail.gif" alt="Demo"></p>
<p>Where can LLM be used in games? Many have approached this question by applying LLMs to NPCs, making them more human-like. Some have even went further and tried to make the game player have a smart companion interactable using speech. However, <strong>do games benefit from such text-based interactions?</strong></p>
<p>During Janaury 2025, I had a chance to take a class at MIT taught by Sony Interaction Entertainment (SIE). Students were lectured on the many research topics the SIE was working on. One of the topics had to do with applying LLMs into games. We were given the opportunity to work on applying LLMs into games, and I decided to leverage LLMs for procedurally generating game scenes.</p>
<p><img src="/assets/project/llm_procedural_generation/game_scene.png" alt="Game Scene"></p>
<h3>Finding the intersections between LLM and games</h3>
<p>Before starting this project, I took some time to discover the intersection between the core functionalities of games and areas LLMs are good at. It is well known that LLMs are proficient in many tasks, but whether its capabilities are effective relies on the target task.</p>
<p>There are many features that consist a good game, but all of them are under the overarching theme, <code>JOY</code>. No matter what the feature may be, it contributes to bringing more JOY to the player.</p>
<p>LLMs are good at multiple tasks. The core feature that allows LLMs to perform all such tasks is its ability to <code>understand the context</code>. It is through this ability that LLMs impact the most in its tasks.</p>
<p>So the question is, how can we leverage LLM's ability to understand context to bring more JOY to the player? I came up with the assumption that players enjoy exploring new worlds. If there is a game feature that motivates the player to explore, it is worth implementing into a game. Thus, I thought of using the power of LLMs to contextually understand what is happening in the game world and generate adequate text descriptions to create pixel art landscapes.</p>
<h3>Strategy</h3>
<p>The LLM was used for two tasks. <strong>First</strong> was to generate a seed text prompt that would be used to generate the initial pixel art landscape scene. A text-to-image model would use this seed text to generate a pixel art landscape image. The <strong>second</strong> task for the LLM was to generate a text description of the connected pixel art landscape image that extends the current player's view of the game world. This time, a image-to-image model would be used to generate a pixel art landscape image. A text prompt and the cropped version of the previously generated image would be used as input to inpaint given image.</p>
<p><img src="/assets/project/llm_procedural_generation/comfyui.png" alt="ComfyUI inpainting"></p>
<p>An example of how the inpainting task would be done is shown above. In this particular example, you can see that it is generating the right side of the image since only the left half of the image is given as input. This means that the player is currently heading to the right side of the current scene. The models used for generating an image and inpainting an image used the same checkpoint and LoRA. The checkpoint used was <a href="https://civitai.com/models/139562/realvisxl-v50?modelVersionId=361593">RealVisXL V5.0</a> and the LoRA used was <a href="https://civitai.com/models/120096/pixel-art-xl">Pixel Art XL V1.1</a>.</p>
<p>The system prompt used for generating and inpainting the images was</p>
<blockquote>
<p>"You are a helpful landscape architecture assistant that designs a game. Your goal is to give a prompt for generating a 2D pixel game stage design. The prompt will be used for generating art with a text to image model. Remember that you should generate a top-view, satellite view image for the pixel game. Only give me prompt sentence for generating the scene. Add no other instructions or title."</p>
</blockquote>
<p>Since I wanted to give players some control over the generated game scenes, I allowed players to input a seed text prompt. The LLM was tasked to create a adequate text prompt for generating the initial pixel art landscape scene. This allowed other images generated through inpainting to maintain the same style as the initial image. For example, if a player inputted a seed text prompt, "city", the LLM would generate a text prompt, "Top-down satellite view of a bustling urban cityscape, with grid-like streets, high-rise buildings, parks, residential areas, and commercial zones, in 2D pixel art style."</p>
<p><img src="/assets/project/llm_procedural_generation/initial_scene.png" alt="Game Scene"></p>
<p>As the player explores the game world, I recorded the player's current grid index as well as their direction of movement. If a player moves to the right and if there is no game scene yet generated for the right side, the current game scene's right half side would be used as an input to the inpainting model. The LLM would be asked to generate a scene using the text prompt that was used to generate the current game scene and be asked to generate a text description of the connected pixel art landscape image based on the player's direction. The LLM would be asked,</p>
<blockquote>
<p>The player is heading <strong>LEFT</strong> from the current scene which was generated with this prompt ‘Top-down satellite view of a bustling urban cityscape, with grid-like streets, high-rise buildings, parks, residential areas, and commercial zones, in 2D pixel art style.’. What would be the adequate prompt for the scene to be seen?”</p>
</blockquote>
<p>The diagram describes the whole inpainting process in detail.</p>
<p><img src="/assets/project/llm_procedural_generation/inpaint.png" alt="Inpainting Process"></p>
<h3>Experiments</h3>
<p>I have experimented with multiple seed prompts for generating the images. Thanks to LLM, the generated game scenes were dynamic but also consistent with the seed prompt. For example, when the seed prompt was 'japan, samurai', the LLM would first generate a scene with traditional japanese architecture, but also generate a scene with Fuji mountain when the player went north.</p>
<p><img src="/assets/project/llm_procedural_generation/japan.png" alt="Japan Scene"></p>
<p>Though the consistency of the connected images was not perfect, the consistency was significantly improved when the game scene was themed with a landscape city. Especially for a game scene generate with a seed prompt 'urban, city', the consistency was very high.</p>
<p><img src="/assets/project/llm_procedural_generation/city.png" alt="City Scene"></p>
<p>The project was created using Unity C# as the game client and FastAPI as the game server. For more information on the project, please visit the <a href="https://github.com/hunkim98/LLM_procedural_terrain">GitHub repository</a>.</p></div></div><div class="max-w-3xl mx-auto mt-16 mb-16"><div style="justify-content:space-between;flex-wrap:wrap;margin-bottom:calc(1.25rem * var(--mantine-scale));color:var(--mantine-color-white);width:100%" class="m_8bffd616 mantine-Flex-root __m__-Rdknm"><a class="border border-white/20 rounded-lg md:hover:scale-[1.01] transition hover:shadow-lg duration-200 cursor-pointer drop-shadow-md p-3 mb-3" style="background-image:url(/assets/background/noise50.png)" href="/projects/korea_blood_donation"><p class="mantine-focus-auto font-sans m_b6d8b162 mantine-Text-root">Previous</p><div style="gap:var(--mantine-spacing-sm);align-items:center" class="m_8bffd616 mantine-Flex-root __m__-Rjddknm"><svg xmlns="http://www.w3.org/2000/svg" width="15" height="15" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="tabler-icon tabler-icon-arrow-left "><path d="M5 12l14 0"></path><path d="M5 12l6 6"></path><path d="M5 12l6 -6"></path></svg><p class="mantine-focus-auto text-sm font-sans font-bold text-left m_b6d8b162 mantine-Text-root"> <!-- -->Visualizing a Nation&#x27;s Blood Donation and Demand</p></div></a></div></div><section></section></article></div><div style="background-image:linear-gradient(to bottom, rgba(0,0,0,0), rgba(0,0,0,1));width:100%;height:calc(5rem * var(--mantine-scale));position:fixed;bottom:0rem" class="z-50"></div><div style="height:1000px;background-image:url(/assets/background/noise30.png)" class="absolute bottom-0 bg-red-400 w-full -z-10"><div style="height:500px;width:100%;background-image:linear-gradient(to bottom, rgba(0,0,0,1), rgba(0,0,0,0));position:absolute;top:0rem;left:0rem" class=""></div></div></main></div></main></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"project":{"title":"Pixel_horizons: Procedural Generation of Pixel Art Landscapes using LLM","date":"2025-01-26","slug":"llm_procedural_generation","author":{"name":"Kim Dong Hun"},"content":"\u003cp\u003e\u003cimg src=\"/assets/project/llm_procedural_generation/thumbnail.gif\" alt=\"Demo\"\u003e\u003c/p\u003e\n\u003cp\u003eWhere can LLM be used in games? Many have approached this question by applying LLMs to NPCs, making them more human-like. Some have even went further and tried to make the game player have a smart companion interactable using speech. However, \u003cstrong\u003edo games benefit from such text-based interactions?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDuring Janaury 2025, I had a chance to take a class at MIT taught by Sony Interaction Entertainment (SIE). Students were lectured on the many research topics the SIE was working on. One of the topics had to do with applying LLMs into games. We were given the opportunity to work on applying LLMs into games, and I decided to leverage LLMs for procedurally generating game scenes.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/project/llm_procedural_generation/game_scene.png\" alt=\"Game Scene\"\u003e\u003c/p\u003e\n\u003ch3\u003eFinding the intersections between LLM and games\u003c/h3\u003e\n\u003cp\u003eBefore starting this project, I took some time to discover the intersection between the core functionalities of games and areas LLMs are good at. It is well known that LLMs are proficient in many tasks, but whether its capabilities are effective relies on the target task.\u003c/p\u003e\n\u003cp\u003eThere are many features that consist a good game, but all of them are under the overarching theme, \u003ccode\u003eJOY\u003c/code\u003e. No matter what the feature may be, it contributes to bringing more JOY to the player.\u003c/p\u003e\n\u003cp\u003eLLMs are good at multiple tasks. The core feature that allows LLMs to perform all such tasks is its ability to \u003ccode\u003eunderstand the context\u003c/code\u003e. It is through this ability that LLMs impact the most in its tasks.\u003c/p\u003e\n\u003cp\u003eSo the question is, how can we leverage LLM's ability to understand context to bring more JOY to the player? I came up with the assumption that players enjoy exploring new worlds. If there is a game feature that motivates the player to explore, it is worth implementing into a game. Thus, I thought of using the power of LLMs to contextually understand what is happening in the game world and generate adequate text descriptions to create pixel art landscapes.\u003c/p\u003e\n\u003ch3\u003eStrategy\u003c/h3\u003e\n\u003cp\u003eThe LLM was used for two tasks. \u003cstrong\u003eFirst\u003c/strong\u003e was to generate a seed text prompt that would be used to generate the initial pixel art landscape scene. A text-to-image model would use this seed text to generate a pixel art landscape image. The \u003cstrong\u003esecond\u003c/strong\u003e task for the LLM was to generate a text description of the connected pixel art landscape image that extends the current player's view of the game world. This time, a image-to-image model would be used to generate a pixel art landscape image. A text prompt and the cropped version of the previously generated image would be used as input to inpaint given image.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/project/llm_procedural_generation/comfyui.png\" alt=\"ComfyUI inpainting\"\u003e\u003c/p\u003e\n\u003cp\u003eAn example of how the inpainting task would be done is shown above. In this particular example, you can see that it is generating the right side of the image since only the left half of the image is given as input. This means that the player is currently heading to the right side of the current scene. The models used for generating an image and inpainting an image used the same checkpoint and LoRA. The checkpoint used was \u003ca href=\"https://civitai.com/models/139562/realvisxl-v50?modelVersionId=361593\"\u003eRealVisXL V5.0\u003c/a\u003e and the LoRA used was \u003ca href=\"https://civitai.com/models/120096/pixel-art-xl\"\u003ePixel Art XL V1.1\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe system prompt used for generating and inpainting the images was\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"You are a helpful landscape architecture assistant that designs a game. Your goal is to give a prompt for generating a 2D pixel game stage design. The prompt will be used for generating art with a text to image model. Remember that you should generate a top-view, satellite view image for the pixel game. Only give me prompt sentence for generating the scene. Add no other instructions or title.\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSince I wanted to give players some control over the generated game scenes, I allowed players to input a seed text prompt. The LLM was tasked to create a adequate text prompt for generating the initial pixel art landscape scene. This allowed other images generated through inpainting to maintain the same style as the initial image. For example, if a player inputted a seed text prompt, \"city\", the LLM would generate a text prompt, \"Top-down satellite view of a bustling urban cityscape, with grid-like streets, high-rise buildings, parks, residential areas, and commercial zones, in 2D pixel art style.\"\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/project/llm_procedural_generation/initial_scene.png\" alt=\"Game Scene\"\u003e\u003c/p\u003e\n\u003cp\u003eAs the player explores the game world, I recorded the player's current grid index as well as their direction of movement. If a player moves to the right and if there is no game scene yet generated for the right side, the current game scene's right half side would be used as an input to the inpainting model. The LLM would be asked to generate a scene using the text prompt that was used to generate the current game scene and be asked to generate a text description of the connected pixel art landscape image based on the player's direction. The LLM would be asked,\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe player is heading \u003cstrong\u003eLEFT\u003c/strong\u003e from the current scene which was generated with this prompt ‘Top-down satellite view of a bustling urban cityscape, with grid-like streets, high-rise buildings, parks, residential areas, and commercial zones, in 2D pixel art style.’. What would be the adequate prompt for the scene to be seen?”\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe diagram describes the whole inpainting process in detail.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/project/llm_procedural_generation/inpaint.png\" alt=\"Inpainting Process\"\u003e\u003c/p\u003e\n\u003ch3\u003eExperiments\u003c/h3\u003e\n\u003cp\u003eI have experimented with multiple seed prompts for generating the images. Thanks to LLM, the generated game scenes were dynamic but also consistent with the seed prompt. For example, when the seed prompt was 'japan, samurai', the LLM would first generate a scene with traditional japanese architecture, but also generate a scene with Fuji mountain when the player went north.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/project/llm_procedural_generation/japan.png\" alt=\"Japan Scene\"\u003e\u003c/p\u003e\n\u003cp\u003eThough the consistency of the connected images was not perfect, the consistency was significantly improved when the game scene was themed with a landscape city. Especially for a game scene generate with a seed prompt 'urban, city', the consistency was very high.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/project/llm_procedural_generation/city.png\" alt=\"City Scene\"\u003e\u003c/p\u003e\n\u003cp\u003eThe project was created using Unity C# as the game client and FastAPI as the game server. For more information on the project, please visit the \u003ca href=\"https://github.com/hunkim98/LLM_procedural_terrain\"\u003eGitHub repository\u003c/a\u003e.\u003c/p\u003e","keyword":"Machine Learning","categories":["Machine Learning"],"thumbnail":"/assets/project/llm_procedural_generation/thumbnail.gif","excerpt":"This project attempts to leverage LLM's ability to contextually understand what is happening in the game world and generate adequate text descriptions to create pixel art landscapes.","data":{"title":"Pixel_horizons: Procedural Generation of Pixel Art Landscapes using LLM","excerpt":"This project attempts to leverage LLM's ability to contextually understand what is happening in the game world and generate adequate text descriptions to create pixel art landscapes.","date":"2025-01-26","author":{"name":"Kim Dong Hun"},"keyword":"Machine Learning","categories":["Machine Learning"],"WIP":false,"thumbnail":"/assets/project/llm_procedural_generation/thumbnail.gif"},"prevPath":"/projects/korea_blood_donation","nextPath":"","prevTitle":"Visualizing a Nation's Blood Donation and Demand","nextTitle":"","isMdx":false}},"__N_SSG":true},"page":"/projects/[slug]","query":{"slug":"llm_procedural_generation"},"buildId":"MfYEUNrk9JLrhJ_NpIfgP","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>